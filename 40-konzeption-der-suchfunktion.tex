\chapter{Konzeption der neuen Suchfunktion}
\label{chap:konzeption-der-suchfunktion}
% TODO: Ergänzen

\subsection{Vektor-Indizierung}
\label{chap:vektorindizes}
Neben einer Volltext-Indizierung können Dokumente in Form von Vektoren indiziert werden.
Dazu werden Dokumente zunächst, wie auch bei der Volltext-Indizierung gecrawlt.
Anschließend durchlaufen die Inhalte der Dokumente ein Preprocessing.
Dieses kann je nach Implementierung variieren.
\myRef{Kapitel}{chap:einspielen-der-daten-in-weaviate} beschreibt, wie in der hier aufgeführten Implementierung das Preprocessing durchgeführt wird.
Das Preprocessing hat den Zweck die Daten an das Schema der Datenbank anzupassen und die Qualität der Daten zu erhöhen.
Außerdem sorgt es für eine kürzere Indizierungszeit.\\

Nach dem Preprocessing werden durch einen Transformer für die Inhalte der Dokumente Vektoren berechnet.
Ein Transformer wird mithilfe von Trainingsdaten darauf trainiert, Vektoren für Wörter zu generieren.
Das trainierte Modell wird nach dem Preprocessing durchlaufen.
Zuletzt werden die Daten in einer Vektordatenbank gespeichert.
Eine Vektordatenbank speichert Daten, wie eine dokumentenbasierte Datenbank.
Dort werden nun sowohl die rohen Daten als auch die Vektoren gespeichert, welche von dem Transformer berechnet wurden.
Die Vektoren haben den Vorteil, dass die Daten in der Datenbank nicht linear gespeichert sind.
Sie sind in einem n-dimensionalen Raum gespeichert, mit dessen Hilfe die semantische Nähe zwischen Dokumenten ausgedrückt werden kann.

\section{Sentence Embeddings}
% TODO: Sentence Transformer, wie Bert
 (\cite{Reimers_Gurevych_2019})

\subsection{Word Embeddings}
Word Embeddings sind eine Vektorrepräsentation von Wörtern.
Zweck von Word Embeddings ist es, semantische Ähnlichkeiten zwischen Wörtern zu verstehen.
Eine einfache Implementierung von Word Embeddings könnte sein, jedem Wort eine zufällige Zahl zuzuordnen.
Wenn die Zahlen zufällig sind, werden damit allerdings keine semantischen Ähnlichkeiten kodiert.
Um herauszufinden, welche Wörter eine semantische Ähnlichkeit besitzen.

\subsection{word2vec}
Word2vec ist ein zwei-Layer neuronales Netzwerk, welches die Semantik von Wörtern lernt.
Für jedes Wort, für welches eine Vektorrepräsentation bestimmt werden soll, benötigt das neuronale Netzwerk einen Input.
Das erste Layer des neuronalen Netzwerkes besteht aus Aktivierungsfunktionen.
Jede Node des Layers summiert die Inputs.
Die Weights der Aktivierungsfunktionen sind am Ende die Vektorrepräsentationen der Wörter.
Die Anzahl der Nodes im Aktivierungslayer bestimmt die Anzahl der Dimensionen im Vektor.
Das nächste Layer summiert wiederum die Werte aus dem Aktivierungslayer und wendet eine SoftMax Funktion an.
Der Output des neuronalen Netzwerkes besteht, so wie der Input, aus so vielen Nodes, wie es Wörter gibt, welche mit dem Netzwerk repräsentiert werden sollen.
Der Output gibt an, mit welcher Wahrscheinlichkeit jedes der Wörter das Folgewort des gegebenen Inputs ist.\\ 

Das neuronale Netzwerk wird mithilfe von Backpropagation trainiert.
Dazu wird Training Data benötigt.
Dieses enthält Sätze, auf welche das neuronale Netzwerk trainiert werden soll.
Aufgabe des neuronalen Netzwerkes ist es, auf Basis eines gegebenen Wortes das nächste Wort im Satz zu bestimmen.
Für ein einfaches Beispiel könnte das Trainingsdatenset aus zwei Einträgen \textit{NLP ist interessant} und \textit{NLP ist komplizit} bestehen.
Da das Trainingsdatenset insgesamt aus vier verschiedenen Wörtern besteht, hat das neuronale Netzwerk genau vier Inputs.
Wenn nun das Folgewort für das Wort \textit{NLP} trainiert werden soll, dann bekommt das neuronale Netzwerk als Input die Werte \textit{1, 0, 0, 0}.
Die Eins bedeutet, dass das Wort NLP aktiviert ist.
Die Nullen bedeuten, dass die anderen Wörter nicht aktiviert sind.
Nun muss das neuronale Netzwerk für diesen Input den Output-Wert berechnen.
Für die Backpropagation wird die Cross-Entropy Loss Function verwendet.
Das Ergebnis ist, dass das trainierte neuronale Netzwerk für das Wört \textit{NLP} lernt, dass das nächste Wort mit der Wahrscheinlichkeit eins das Wort \textit{ist} ist.
Die gelernten Weights bilden einen Vektor, welcher in einer Vektordatenbank gespeichert werden kann.
Das Ergebnis ist eine Vektordatenbank, welche Informationen darüber enthält, welche Wörter semantisch ähnlich sind, und welche nicht.

\subsection{seq2seq}
Ein seq2seq-Modell soll eine Sequenz auf eine andere Sequenz anhand von erlernten Regeln mappen.
Dazu wird die Input-Sequenz zunächst in eine modell-interne Sequenz encodiert, der Context Vector, und anschließend in die Output-Sequenz dekodiert.
Wenn eine Wortsequenz in eine andere Wortsequenz umgewandelt werden soll, dann müssen die Input-Wörter zunächst durch ein Word Embedding Layer verarbeitet werden, welches die Wörter in Word Embeddings konvertiert.
Auf das Word Embedding Layer folgen eine beliebige Anzahl von LSTM-Layers.
Die Anzahl der Nodes der LSTM-Layer entspricht der Anzahl der Nodes im Word Embedding Layer.
Zusätzlich kann jede Node in jedem LSTM-Layer mehrere LSTM Netzwerke besitzen.
Die Weights und Biases der LSTM Netzwerke bilden den Output und damit den Context Vector.
Die bisher beschriebene Architektur wird als Encoder bezeichnet.
Die LSTM Netzwerke des Encoders werden im Decoder gespiegelt.
Die Outputs des oberen LSTM Layers des Decoders werden in ein Fully Connected Layer gegeben und durchlaufen eine Softmax Funktion.
Der Output des Decoders sind die Wahrscheinlichkeiten, mit welchen die möglichen Wörter der Ziel-Sequenz dem Input entsprechen. 

Der Context Vector kann in einer Vektordatenbank gespeichert werden, so wie Word Embeddings in einer Vektordatenbank gespeichert werden können.
Das hat zur Folge, dass die Ähnlichkeit von Sätzen genauso wie die Ähnlichkeit von Wörtern ermittelt werden kann.

\subsection{Scoring Algorithmen}

\section{Cosine Similarity}
% TODO: 

\section{Hierarchical Navigable Small Worlds (HNSW)}
HNSW ist ein Nearest Neighbor Suchalgorithmus.
Der Algorithmus findet die ähnlichsten Nachbar-Vektoren zu einem gegebenen Input-Vektor.
Um den Algorithmus umzusetzen werden Skip Lists und Navigable Small Worlds verwendet.
Skip Lists sind klassische Linked Lists, welche aus mehreren Schichten besteht.
Die unterste Schicht entspricht der originalen Linked List und beinhaltet alle Daten.
Jedes höhere Layer beinhaltet nur ein Subset der Nodes des darunterliegenden Layers.
Wenn nun eine Suche in der Skip List durchgeführt wird, dann wird das oberste Layer zuerst durchsucht.
Das oberste Layer wird sequenziell durchlaufen.
Es wird bei dem ersten Element des obersten Layers begonnen.
Solange das aktuelle Element kleiner ist als das gesuchte Element, wird das nächste Element untersucht.
Wenn das Element n dem gesuchten Element entspricht, dann wurde das gesuchte Element gefunden und wird zurückgegeben.
Wenn das nächste Element n+1 größer ist als das gesuchte Element, dann wird das aktuelle Element n in dem aktuellen Layer gefunden.
Da dieses Element noch nicht dem gesuchten Element entspricht, wird nun im tieferen Layer weitergesucht, in welchem mehr Elemente vorhanden sind.
Denn das gesuchte Element liegt zwischen dem gefundenen Element n und dem untersuchten Element n+1.
Der gleiche Algorithmus wird rekursiv für jedes Layer durchgeführt, bis das unterste Layer erreicht wurde.
Wurde am Ende des untersten Layers kein Element gefunden, dann konnte das Element in der Skip List nicht gefunden werden (\cite{10.5555/93711}).\\

% TODO: Navigable Small Worlds (\cite{Malkov_Yashunin_2020})\\ 

\section{Suchalgorithmen}
Im Folgenden werden zuerst gängige Suchalgorithmen erklärt, welche in nahezu jeder Suchfunktion zu finden sind.
Danach wird im speziellen auf die strukturierte Suche und die semantische Suche eingegangen.
Die semantische Suche wird später bei der Implementierung einer Suchfunktion verwendet werden.

\subsection{Die semantische Suche}
Unter einer semantischen Suche wird im Allgemeinen verstanden, dass die Suche nicht nur eine syntaktische Suche einer Zeichenkette ist, sondern auch Techniken, wie technische Analysen verwendet, um die Bedeutung der Sucheingabe nachzuvollziehen (\cite{Dengel_2012}).
Eine semantische Suche hat den Zweck, die Ähnlichkeit und Beziehungen zwischen Wörtern zu verstehen.
Sie kennt Homonyme, Synoyme und Antonyme von Wörtern.  
So wird durch sie beispielsweise die Ähnlichkeit von den Wörtern \textit{rollout} und \textit{deployment} abgebildet, und dass diese Wörter oft im gleichen Kontext verwendet werden.\\

Die technische Umsetzung einer semantischen Suche kann auf verschiedene Arten erfolgen.
Eine Möglichkeit zur Umsetzung einer semantischen Suche ist die Verwendung von Transformers und Vektordatenbanken.
Um zu verstehen, welche Wörter kontextuell zusammengehören, werden hier die Wörter in einem n-dimensionalen Raum positioniert.
Wörter, die sich sehr ähnlich sind, also im gleichen Kontext verwendet werden, haben in diesem n-dimensionalen Raum eine geringe Distanz zueinander.
Wörter, die sich eher unähnlich sind, wie \textit{"rollout" und "API"}, haben eine größere Distanz.
Der Vorteil einer semantischen Suche ist, dass der genaue Begriff, welcher gesucht wird nicht bekannt ist.
Wenn sich der Nutzer also über ein Thema informieren möchte, mit welchem er nicht gut vertraut ist, dann kann die semantische Suche hilfreich sein.
Denn der Nutzer kann nun einen Begriff eingeben, der zu dem Thema passt, und den er kennt.
Er findet anschließend Dokumente, welche vielleicht nicht genau diesen Begriff beinhalten, aber welche thematisch dennoch ähnlich sind.
Genau dieser Vorteil soll bei der Implementierung später genutzt werden.\\

Um eine semantische Suche zu implementieren, werden die Technologien von Transformern und Vektordatenbanken verwendet.
Ein Transformer erhält als Input eine große Menge an Text und mappt die einzelnen Wörter auf einen Vektor einer beliebigen Länge.
Der Vektor, der am Ende herauskommt, beschreibt die Position des Wortes in dem n-dimensionalen Raum.
Der Vektor beschreibt gewissermaßen, wie stark ein Wort in eine abstrakte Kategorie einzuordnen ist.
Jeder Wert im Vektor entspricht einer Kategorie.
Mithilfe der Vektoren können verschiedene Wörter hinsichtlich ihrer Ähnlichkeit analysiert werden.
Ähnliche Wörter habe eine große räumliche Nähe, während zwei Wörter, die in vollkommen unterschiedlichen Kontexten verwende werden eine sehr große Distanz im Raum besitzen.
Nehmen wir für ein Beispiel einen dreidimensionalen Raum an.
Die X-Achse ist beschriftet mit dem Wort „Tier“, die Y-Achse ist beschriftet mit dem Wort „Computer“ und die Z-Achse ist beschriftet mit dem Wort „Mensch“.
Nun geben wir einem Transformer das Wort „Katze“, und der Transformer berechnet einen dreidimensionalen Vektor, welcher das Wort „Katze“ im Raum positioniert.
Weil eine Katze ein Tier ist, ist der X-Wert des Vektors eins.
Der Wert eins bedeutet, dass das Wort vollständig zu dieser Kategorie gehört.
Da eine Katze überhaupt nichts mit einem Computer zu tun hat, ist der Y-Wert des Vektors 0.\\

Nun ist eine Katze kein Mensch, aber eine Katze ist ein Haustier von Menschen.
Es ist denkbar, dass die Wörter Katze und Mensch oft im gleichen Kontext verwendet werden, sodass der Wert bei 0,3 liegen könnte.
Damit der Transformer einen Vektor berechnen kann, braucht er eine Menge Daten.
Diese Daten erhält er aus vielen Texten.
Werden zwei Wörter oft im gleichen Text genannt oder kommen zwei Wörter in vielen Texten sehr nahe beieinander vor, dann geht der Transformer davon aus, dass die beiden Wörter ähnlich sind, und berechnet ähnliche Vektoren.
Zuvor müssen die Texte allerdings bereitgestellt werden.
Dazu kann beispielsweise das Internet gecrawlt werden.
Die Ergebnisse des Transformers werden in einer Vektordatenbank gespeichert.
Eine Vektordatenbank ist eine Datenbank, welche Vector Embeddings, also ein Objekt als Key und dessen Vektor als Value speichert.
Bei dem Objekt kann es sich um Wörter handeln, dann wird auch von Word Embeddings gesprochen.
Es können aber auch Daten andere Daten, wie Bilder, Videos oder Audio gespeichert werden.
Der Zweck von Vektordatenbanken ist es, Daten nicht einfach linear zu speichern, sondern in einem Raum.
Die Distanz zwischen zwei Einträgen in diesem Raum beschreibt dessen Ähnlichkeit.
Genau diese Informationen nutzen semantische Suchen.

\section{Wahl der Suchfunktion}
% TODO: Roter Faden fehlt in diesem Kapitel. Kurze Einleitung zur Darstellung der Argumentation und Fazit hinzufügen

Die vergangenen Kapitel haben verschiedene Indizierungs- und Suchalgorithmen dargestellt.
Diese werden nun betrachtet, um darzustellen, warum semantische Suche auf Basis von Sentence Transformern eine sinnvolle alternative zu der bestehenden Confluence Suche ist.\\

Die Confluence Suche verwendet laut Dokumentation Apache Lucene (\cite{Confluence_Ranking}).
Laut der Dokumentation von Apache Lucene, verwendet dieses einen Scoring Algorithmus, welcher sowohl auf VSM als auch auf Boolean Models basiert (\cite{Lucene_Scoring}).
Im Information Retrieval sind Boolean Models jene Scoring Algorithmen, welche auf boolscher Algebra basieren.
Aus der Dokumentation von Apache Lucene und aus dem genannten Paper geht ebensfalls hervor, dass die Vektoren des VSM mit tf-idf berechnet werden.

In einem Paper von Choudhary et. al. wird ein Document Retrieval System entwickelt (\cite{Choudhary_Guttikonda_Chowdhury_Learmonth_2020}).
Das Document Retrieval System verwendet Bert zur Generierung von Embeddings.
Der Scoring Algorithmus des Systems kombiniert Bert und tf-idf, um den Score zu berechnen.
Laut dem Paper bietet eine Kombination aus tf-idf und Bert zur Implementierung eines Document Retrieval Systems signifikante Performance Verbesserungen gegenüber einem Document Retrieval System, welches lediglich tf-idf verwendet.
Die Performance Verbesserung wird in dem Paper an dem MS Marco Datensatz gemessen.\footnote{\url{https://microsoft.github.io/msmarco/}}\\

In einem Paper von Karpukhin et. al. wird Open-Domain Question Answering untersucht (\cite{Karpukhin_Oguz_Min_Lewis_Wu_Edunov_Chen_Yih_2020}).
Dazu wird ein Dense Passage Retriever entwickelt.
Ein Dense Passage Retriever bestimmt den Textabschnitt eines gegebenen Textes, welcher mit größter Wahrscheinlichkeit eine Antwort auf eine gestellte Frage beinhaltet.
Der Input für einen Dense Passage Retriever sind Dokumente, welche zuvor mithilfe eines Scoring Algorithmus aus einem Index extrahiert wurden.
Das Paper zeigt, dass das entwickelte Passage Retrieval System besser darin ist relevante Textabschnitte zu finden als der BM25 Algorithmus. 
Das Paper nennt die semantische Verknüpfung von Synonymen und Paraphrasierungen mit unterschiedlichen Tokens als Vorteil gegenüber BM25 und auch TF-IDF.\\

% TODO: Lexical Gap thematisieren
% (\cite{Berger_Caruana_Cohn_Freitag_Mittal_2000})

Weaviate ist eine Vektordatenbank, welche Sentence Transformer nutzt, um Dokumente zu indizieren, und HNSW als Scoring-Algorithmus zur Ermittlung des passendsten Dokumentes.
Die Performance des Scoring Algorithmus wurde durch Weaviate anhand von Benchmarks überprüft.
Dabei wird der Recall für das erste Element, die ersten zehn Elemente und die ersten 100 Elemente gemessen.
Zusätzlich ist angegeben, wie Weaviate konfiguriert ist und welcher Datensatz verwendet wird.
Die Benchmark zeigt, dass HNSW für den Datensatz SIFT1M einen Recall-Wert von 90.91\% besitzt (\cite{Weaviate_Benchmark}).

Neben den direkten Vorteilen einer semantischen Suche gegenüber Suchfunktionen, wie eine Keyword Search, gibt es weitere indirekte Vorteile eine semantischen Suche auf Grundlage von Vector Space Models.
Vector Space Models sind Multimodal.
Es können also in einem Vector Space Model nicht nur Informationen über ein Medium, wie Fließtext, gespeichert werden, sondern neben Fließtext können gleichzeitig auch andere Medien, wie Code, Bilder, Audiodateien etc. gespeichert werden.
Und anders als bei einer gewöhnlichen Datenbank, in welcher mehrere verschiedene Medien gespeichert werden können, beispielsweise eine relationale Datenbank, können Vector Space Models diese verschiedenen Medien miteinander semantisch verknüpfen.\\

\section{Weitere Ansätze}
% TODO: 

\subsection{Retrieval Augmented Generation}
Neben einer Suchfunktion können System entwickelt werden, welche eine Question-Answering Funktionalität bieten.
Eine Question-Answering Funktionalität nimmt eine Frage des Nutzers entgegen, und liefert eine Antwort.
Solche Systeme basieren auf Retrieval Augmented Generation.
Das bedeutet, dass auf Grundlage der Frage des Nutzers zuerst eine Suche durchgeführt wird.
Diese Suche liefert relevante Dokumente zum Beantworten der Frage.
Nun werden die, für die Frage relevantesten, Stellen der Dokumente mithilfe eines Large Language Models extrahiert und umformuliert.
Das Ergebnis ist eine Antwort auf die Frage des Nutzers. 

% TODO: Generative Search

\subsection{Semantische Netze}
Es besteht die Möglichkeit ein explizites semantisches Netz heranzuziehen, und so die Zusammenhänge von Wörtern abzubilden.
Ein semantisches Netz ist eine Menge aus Aussagen in der Form \textit{Subjekt Prädikat Objekt}.
Anhand dieser Aussagen wird ein Graph aus Beziehungen zwischen Wörtern hergestellt (\cite{Lehmann}).\\

% TODO: Erläuterende Grafik + Beispiel.\\

Das hat den Vorteil, dass ein solches explizites semantisches Netz von Menschen erstellt wurde, und damit eine hohe Qualität der Daten einhergeht.
Denn an der Erstellung von semantischen Netzen sind mehrere Menschen beteiligt, welche zuerst einen Konsens über die Eigenschaften und Zusammenhänge von Wörtern schaffen müssen.
Der Nachteil dieser Methode ist der große Arbeitsaufwand, welcher mit der Erstellung eines solchen semantischen Netzes einhergeht.
Ein weiterer Nachteil ist die mögliche Unvollständigkeit, welche ein solches semantisches Netz besitzen könnte.
Eine Wissensdatenbank, welche für ein Projekt erstellt wurde, kann Definitionen von Begriffen beinhalten, welche in allgemeinen semantischen Netzen nicht vorhanden sind.
Bei der Umsetzung einer semantischen Suche mithilfe eines semantischen Netzes müsste also zuerst ein allgemeines semantisches Netz herangezogen werden, beispielsweise von DBPedia.
Anschließend müsste dieses semantische Netz um die neuen Definitionen, welche nur im Kontext des Projektes gelten, erweitert werden.\\


% \subsection{Document Classification und Filter verwenden}
% TODO:

\subsection{Verbesserung der Suchfunktion durch Traceability (insb. für Feature Location und Bug Localization)}
Indem die Codebase, welche zu der Wissensdatenbank in Verbindung steht, ebenfalls indiziert wird, kann eine Traceability zwischen Code und Wissensdatenbank hergestellt werden.
Damit können Feature Location und Bug Localization umgesetzt werden.
So kann die Suchfunktion nicht nur zum Durchsuchen der Wissensdatenbank verwendet werden, sondern auch zum Durchsuchen der Codebase.
Und wenn in der Suchfunktion nach einem Feature gesucht wird, dann kann neben der Spezifikation in der Wissensdatenbank auch gleich der Einstiegspunkt im Code gefunden werden.
Damit verbessert sich wiederum die Wartbarkeit der Software, da das Ändern von Features oder korrigieren von Bugs erleichtert wird.\\
% TODO: Quellen

Wenn ein Softwareentwickler ein bestehendes Feature anpassen muss, dann muss er zunächst den Einstiegspunkt im Code kennen.
Sind keine weiteren Informationen vorhanden, dann muss der Softwareentwickler dazu den Code manuell durchsuchen.
Mithilfe von Techniken aus dem Natural Language Processsing, wie Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA) und Vector Space Models kann dieser Prozess automatisiert werden (\cite{Dit_Revelle_Gethers_Poshyvanyk_2011}).
Mithilfe eines gemeinsamen Index von den Inhalten der Wissensdatenbank und dem Quellcode lässt sich ein Zusammenhang zwischen Feature-Spezifikation und Klassen im Quellcode herstellen.
Die Herangehensweise erfordert eine Möglichkeit beide Datenquellen auf die gleiche Art und Weise zu indizieren.
Dazu werden LSI und Vector Space Models verwendet.\citetitle{Antoniol_Canfora_Casazza_DeLucia_2000}

% \subsection{Inhaltsverzeichnisbezogen durchsuchen}
% TODO: