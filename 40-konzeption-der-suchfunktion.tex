\chapter{Konzeption der neuen Suchfunktion}
\label{chap:konzeption-der-suchfunktion}
Dieses Kapitel dient der Konzeption einer neuen Suchfunktion.
Die neue Suchfunktion gebraucht eine andere Technologie als die Suchfunktion von Confluence.
Während die Confluence-Suche ein Boolean Model und TF-IDF nutzt, ist die neue Suchfunktion eine semantische Suche.
Im Folgenden wird zuerst erörtert, was eine semantische Suche ist.
Dann wird erklärt, warum die neue Suchfunktion eine bessere Performance erzielen soll als die bestehende Confluence-Suche.
Anschließend werden die Technologien erläutert, welche benötigt werden, um die konzipierte semantische Suche zu implementieren.
Dies umfasst Sentence Transformer.
Sie indizieren Dokumente, indem sie die Sätze in Vektoren transformieren, welche die Semantik der Sätze abbilden.
Außerdem umfasst dies Vektorindizes, welche die generierten Vektoren persistent speichern.
Zuletzt umfasst dies auch die Bestimmung der relevantesten Dokumente.
Dazu werden Hierarchical Navigable Small Worlds (HNSW) benutzt.
Nachdem die Technologien erörtert wurden, wird die Umsetzung der Technologien mithilfe von Weaviate erläutert.

\section{Semantische Suche}
Unter einer semantischen Suche wird im Allgemeinen verstanden, dass die Suche nicht nur eine syntaktische Suche einer Zeichenkette ist.
Stattdessen verwenden semantische Suchen Techniken, um die Bedeutung der Sucheingabe nachzuvollziehen (\cite{Dengel_2012}).
Eine semantische Suche hat den Zweck, die Ähnlichkeit und die Beziehungen zwischen Wörtern zu verstehen.
Sie kennt Homonyme, Synoyme und Antonyme von Wörtern.
Anders als bei den bereits betrachteten Volltext-Indizes. 
So wird durch sie beispielsweise die Ähnlichkeit von den Wörtern \textit{rollout} und \textit{deployment} abgebildet, und dass diese Wörter oft im gleichen Kontext verwendet werden.\\

Eine Möglichkeit zur Umsetzung einer semantischen Suche ist die Verwendung von Transformers und Vektordatenbanken.
Um zu verstehen, welche Wörter kontextuell zusammengehören, werden hier die Wörter in einem n-dimensionalen Raum positioniert.
Wörter, die sich sehr ähnlich sind, also im gleichen Kontext verwendet werden, haben in diesem n-dimensionalen Raum eine geringe Distanz zueinander.
Wörter, die sich eher unähnlich sind, wie \textit{"rollout" und "API"}, haben eine größere Distanz.
Der Vorteil einer semantischen Suche ist, dass der genaue Begriff, welcher gesucht wird nicht bekannt sein muss.
Auch ein Synonym des gesuchten Begriffes reicht aus, um das passende Dokument zu finden.
Wenn sich der Nutzer also über ein Thema informieren möchte, mit welchem er nicht gut vertraut ist, dann kann die semantische Suche hilfreich sein.
Denn der Nutzer kann nun einen Begriff eingeben, der zu dem Thema passt, und den er kennt.
Er findet anschließend Dokumente, welche vielleicht nicht genau diesen Begriff beinhalten, aber welche thematisch dennoch ähnlich sind.
Genau dieser Vorteil soll bei der Implementierung später genutzt werden.\\

Ein Sentence Transformer erhält als Input eine große Menge an Text und mappt die einzelnen Wörter auf einen Vektor einer bestimmten Länge.
Die Länge wird durch das Modell des Transformers vorgegebenen.
Der Vektor, der am Ende herauskommt, beschreibt die Position des Wortes in dem n-dimensionalen Raum.
Der Vektor beschreibt gewissermaßen, wie stark ein Wort in eine abstrakte Kategorie einzuordnen ist.
Jeder Wert im Vektor entspricht einer Kategorie.
Mithilfe der Vektoren können verschiedene Wörter hinsichtlich ihrer Ähnlichkeit analysiert werden.
Ähnliche Wörter habe eine große räumliche Nähe, während zwei Wörter, die in vollkommen unterschiedlichen Kontexten verwende werden eine sehr große Distanz im Raum besitzen.
Nehmen wir für ein Beispiel einen dreidimensionalen Raum an.
Die x-Achse ist beschriftet mit dem Wort „Tier“, die y-Achse ist beschriftet mit dem Wort „Computer“ und die z-Achse ist beschriftet mit dem Wort „Mensch“.
Nun geben wir einem Transformer das Wort „Katze“, und der Transformer berechnet einen dreidimensionalen Vektor, welcher das Wort „Katze“ im Raum positioniert.
Weil eine Katze ein Tier ist, ist der X-Wert des Vektors eins.
Der Wert eins bedeutet, dass das Wort vollständig zu dieser Kategorie gehört.
Da eine Katze überhaupt nichts mit einem Computer zu tun hat, ist der Y-Wert des Vektors 0.\\

Nun ist eine Katze kein Mensch, aber eine Katze ist ein Haustier von Menschen.
Es ist denkbar, dass die Wörter Katze und Mensch oft im gleichen Kontext verwendet werden, sodass der Wert bei 0,3 liegen könnte.
Damit der Transformer einen Vektor berechnen kann, braucht er eine Menge Daten.
Diese Daten erhält er aus vielen Texten.
Werden zwei Wörter oft im gleichen Text genannt oder kommen zwei Wörter in vielen Texten sehr nahe beieinander vor, dann geht der Transformer davon aus, dass die beiden Wörter ähnlich sind, und berechnet ähnliche Vektoren.
Zuvor müssen die Texte allerdings bereitgestellt werden.
Dazu kann beispielsweise das Internet gecrawlt werden.
Die Ergebnisse des Transformers werden in einer Vektordatenbank gespeichert.
Eine Vektordatenbank ist eine Datenbank, welche Vector Embeddings, also ein Objekt als Key und dessen Vektor als Value speichert.
Bei dem Objekt kann es sich um Wörter handeln, dann wird auch von Word Embeddings gesprochen.
Es können aber auch Daten andere Daten, wie Bilder, Videos oder Audio gespeichert werden.
Der Zweck von Vektordatenbanken ist es, Daten nicht einfach linear zu speichern, sondern in einem Raum.
Die Distanz zwischen zwei Einträgen in diesem Raum beschreibt dessen Ähnlichkeit.
Genau diese Informationen nutzen semantische Suchen.

\section{Wahl der neuen Suchfunktion}
Dieses Kapitel betrachtet verschiedene Quellen, um zu zeigen, welches Verbesserungspotenzial die Verwendung von Sentence Transformern im Document Retrieval bietet.
Zuerst wird anhand der Arbeit von Choudhary et al. (\citeyear{Choudhary_Guttikonda_Chowdhury_Learmonth_2020}) gezeigt, wie das Scoring im Document Retrieval durch die Verwendung von Bert verbessert wird.
Anschließend wird die Arbeit von Karpukhin et al. (\citeyear{Karpukhin_Oguz_Min_Lewis_Wu_Edunov_Chen_Yih_2020}) herangezogen, um zu verstehen, warum es zu einer Verbesserung im Document Retrieval kommt, wenn Sentence Transformer verwendet werden.
Die Argumente von Karpukhin et al. werden dann durch Berger et al. (\citeyear{Berger_Caruana_Cohn_Freitag_Mittal_2000}) unterstützt.
Zuletzt werden die Argumente für die Verwendung von Sentence Transformern ebenfalls mithilfe von Benchmark-Werten gestützt.\\

Choudhary et al. (\citeyear{Choudhary_Guttikonda_Chowdhury_Learmonth_2020}) entwickelten ein Document Retrieval System, welches Bert zur Generierung von Embeddings verwendet.
Der Scoring-Algorithmus des Systems kombiniert Bert und TF-IDF, um den Score zu berechnen.
Laut dem Paper bietet eine Kombination aus TF-IDF und Bert zur Implementierung eines Document Retrieval Systems signifikante Performance Verbesserungen gegenüber einem Document Retrieval System, welches lediglich TF-IDF verwendet.
Die Arbeit von Choudhary et al. bieten damit einen ersten Anhaltspunkt darauf, dass Document Retrieval Systeme auf Basis von TF-IDF mithilfe von Sentence Transformern verbessert werden können.
Die Performance Verbesserung wird in dem Paper an dem MS Marco Datensatz gemessen.\footnote{\url{https://microsoft.github.io/msmarco/}}
Die nachgewiesene Verbesserung ist Grund zur Annahme, dass auch die Confluence-Suche durch die Verwendung von Sentence Transformern verbessert werden kann.
Denn die Confluence-Suche verwendet laut Dokumentation Apache Lucene (\cite{Confluence_Ranking}).
Und laut der Dokumentation von Apache Lucene, verwendet dieses einen Scoring-Algorithmus, welcher sowohl auf VSM als auch auf Boolean Models basiert (\cite{Lucene_Scoring}).
Im Information Retrieval sind Boolean Models jene Scoring-Algorithmen, welche auf boolscher Algebra basieren.
Aus der Dokumentation von Apache Lucene und aus dem genannten Paper geht ebensfalls hervor, dass die Vektoren des VSM mit TF-IDF berechnet werden.\\

Karpukhin et al. (\citeyear{Karpukhin_Oguz_Min_Lewis_Wu_Edunov_Chen_Yih_2020}) entwickelten einen Dense-Passage Retriever im Kontext des Open-Domain Question Answering.
Ein Dense Passage Retriever bestimmt den Textabschnitt eines gegebenen Textes, welcher mit größter Wahrscheinlichkeit eine Antwort auf eine gestellte Frage beinhaltet.
Der Input für einen Dense Passage Retriever sind Dokumente, welche zuvor mithilfe eines Scoring-Algorithmus aus einem Index extrahiert wurden.
Karpukhin et al. (\citeyear{Karpukhin_Oguz_Min_Lewis_Wu_Edunov_Chen_Yih_2020}) zeigen, dass das entwickelte Passage-Retrieval System besser darin ist relevante Textabschnitte zu finden als der BM25 Algorithmus. 
Sie nennen die semantische Verknüpfung von Synonymen und Paraphrasierungen mit unterschiedlichen Tokens als Vorteil gegenüber BM25 und auch TF-IDF.
Das Argument lässt sich auf das Thema dieser Arbeit übertragen.
Das bedeutet, dass Sentence Transformer gegenüber TF-IDF und BM25 den Vorteil haben, dass sie Synonyme und Paraphrasierungen semantisch verknüpfen können.
Damit lässt sich die bessere Performance von Sentence Transformern gegenüber TF-IDF und BM25 im Kontext des Document Retrieval begründen.\\

Berger et al. (\citeyear{Berger_Caruana_Cohn_Freitag_Mittal_2000}) bezeichnen die Problematik der Synonyme im Kontext von TF-IDF und BM25 als die Lexical Gap.
Sie erklären, dass Scoring-Algorithmen, wie TF-IDF lediglich eine lexikalische Analyse der Dokumente und der Sucheingabe durchführen.
Dies ermöglicht jedoch nicht die Abbildung von Synonymen.
Wenn der Nutzer beispielsweise nach \textit{NLP} sucht, dann erwartet der Nutzer Dokumente über Natural Language Processing.
Wenn nun der Begriff \textit{NLP} als solches aber nicht explizit in den Dokumenten genannt wird, sondern lediglich in der ausgeschriebenen Form \textit{Natural Langauge Processing}, dann können die Dokumente mithilfe des TF-IDF Scoring-Algorithmus nicht gefunden werden.\\

Die genannten Quellen zeigen, dass semantische Suchen grundsätzlich den Vorteil gegenüber TF-IDF und BM25 besitzen, dass sie Synonyme verstehen können, und damit relevante Dokumente finden können, welche mit TF-IDF und BM25 nicht gefunden werden können.
Nun muss untersucht werden, ob dieser Vorteil sich auch in besseren Messwerten im Document Retrieval wiederspiegelt.
Dazu werden nun die Benchmark-Werte für die semantische Suche von Weaviate herangezogen. 
Weaviate ist eine Vektordatenbank, welche Sentence Transformer nutzt, um Dokumente zu indizieren, und HNSW als Scoring-Algorithmus zur Ermittlung des passendsten Dokumentes.
Die Performance des Scoring-Algorithmus wurde durch Weaviate anhand von Benchmarks überprüft.
Dabei wird der Recall für das erste Element, die ersten zehn Elemente und die ersten 100 Elemente gemessen.
Zusätzlich ist angegeben, wie Weaviate konfiguriert ist und welcher Datensatz benutzt wird.
Die Benchmark zeigt, dass HNSW für den Datensatz SIFT1M einen Recall-Wert von 90.91\% besitzt (\cite{Weaviate_Benchmark}).
Nachdem nun die Vorteile von semantischen Suchen beschrieben wurden, werden als Nächstes die Technologien beschrieben, welche notwendig sind, um eine solche semantische Suche zu implementieren.

\section{Sentence Transformer}
\label{chap:sentence-transformer}
% TODO: Sentence Transformer, wie Bert
% (\cite{Reimers_Gurevych_2019})

Ein Sentence Transformer transformiert eine Aneinanderreihung von Tokens beliebiger Länge in einen Vektor einer vorgegebenen Länge.
Der Aneinanderreihung der Tokens kann dabei ein Satz sein, ein Teil eines Satzes oder auch ein ganzer Paragraph.
Die Länge des Vektors wird durch die Architektur des Sentence Transformers vorgegeben.
Um eine Aneinanderreihung von Tokens in einen Vektor zu transformieren, braucht der Sentence Transformer für jedes Wort in dessen Wortschatz einen Input.
Da ein neuronales Netzwerk keine Wörter verarbeiten kann, sondern nur Zahlen, müssen die Wörter zuerst in Zahlen umgewandelt werden.
Dies erfolgt mithilfe von Word Embeddings und wird in \myRef{Kapitel}{chap:word-embeddings} erläutert.\\

Wenn nun die Semantik eines Satzes in einen Vektor transformiert werden soll, dann ist die Reihenfolge der Wörter in dem Satz wichtig zu beachten, um die Semantik des Satzes abzubilden.
Der Satz \textit{Software-Entwickler verwenden die Spezifikation, um Features korrekt zu implementieren} besitzt die gleichen Wörter, wie der Satz \textit{Features verwenden die Spezifikation, um Software-Entwickler korrekt zu implementieren}.
Dennoch ist die Sematik zwischen den Sätzen nicht die gleiche.
Das bedeutet, dass ein Sentence Transformer die Positionen der einzelnen Wörter in dem Vektor kodieren muss, um die Sematik des Satzes korrekt abzubilden.
Die Kodierung der Positionen der Wörter in einem Satz wird als Positional Encoding bezeichnet und wird in \myRef{Kapitel}{chap:positional-encoding} erklärt.
Zuletzt wird Self-Attention auf die Kodierung der Wörter angewendet.
Dieser Schritt wird in \myRef{Kapitel}{chap:self-attention} erklärt.
Die beschriebenen Schritte bilden zusammengefasst den Encoder eines Transformers.
% TODO: Sentence Transformer SBert
Er generiert einen Vektor einer fixen Länge für eine beliebig lange Eingabe.
Der generierte Vektor wird auch als Context Vector bezeichnet.
Bert wurde entwickelt, um eine gute automatische Übersetzung zwischen Sprachen zu ermöglichen.
Die Architektur von Bert umfasst neben dem Encoder, welcher einen Context Vector generiert, auch einen Decoder, welcher diesen Context Vector wieder in einen Satz einer anderen Sprache umwandelt.
Im Rahmen dieser Arbeit ist nur die Betrachtung des Encoders relevant.
Denn dieser generiert bereits die Context Vectors, und diese werden anschließend in einer Vektordatenbank gespeichert.

\subsection{Word Embeddings}
\label{chap:word-embeddings}
Word Embeddings sind eine Vektorrepräsentation von Wörtern.
Zweck von Word Embeddings ist es, semantische Ähnlichkeiten zwischen Wörtern zu verstehen.
Eine einfache Implementierung von Word Embeddings könnte sein, jedem Wort eine zufällige Zahl zuzuordnen.
Wenn die Zahlen zufällig sind, werden damit allerdings keine semantischen Ähnlichkeiten kodiert.
Um herauszufinden, welche Wörter eine semantische Ähnlichkeit besitzen wird word2vec benutzt, um Word Embeddings zu generieren.\\

Word2vec ist ein zwei-Layer neuronales Netzwerk, welches die Semantik von Wörtern lernt.
Für jedes Wort, für welches eine Vektorrepräsentation bestimmt werden soll, benötigt das neuronale Netzwerk einen Input.
Das erste Layer des neuronalen Netzwerkes besteht aus Aktivierungsfunktionen.
Jede Node des Layers summiert die Inputs.
Die Weights der Aktivierungsfunktionen sind am Ende die Vektorrepräsentationen der Wörter.
Die Anzahl der Nodes im Aktivierungslayer bestimmt die Anzahl der Dimensionen im Vektor.
Das nächste Layer summiert wiederum die Werte aus dem Aktivierungslayer und wendet eine SoftMax Funktion an.
Der Output des neuronalen Netzwerkes besteht, so wie der Input, aus so vielen Nodes, wie es Wörter gibt, welche mit dem Netzwerk repräsentiert werden sollen.
Der Output gibt an, mit welcher Wahrscheinlichkeit jedes der Wörter das Folgewort des gegebenen Inputs ist.\\ 

Das neuronale Netzwerk wird mithilfe von Backpropagation trainiert.
Dazu wird Training Data benötigt.
Dieses enthält Sätze, auf welche das neuronale Netzwerk trainiert werden soll.
Aufgabe des neuronalen Netzwerkes ist es, auf Basis eines gegebenen Wortes das nächste Wort im Satz zu bestimmen.
Für ein einfaches Beispiel könnte das Trainingsdatenset aus zwei Einträgen \textit{NLP ist interessant} und \textit{NLP ist komplizit} bestehen.
Da das Trainingsdatenset insgesamt aus vier verschiedenen Wörtern besteht, hat das neuronale Netzwerk genau vier Inputs.
Wenn nun das Folgewort für das Wort \textit{NLP} trainiert werden soll, dann bekommt das neuronale Netzwerk als Input die Werte \textit{1, 0, 0, 0}.
Die Eins bedeutet, dass das Wort NLP aktiviert ist.
Die Nullen bedeuten, dass die anderen Wörter nicht aktiviert sind.
Nun muss das neuronale Netzwerk für diesen Input den Output-Wert berechnen.
Für die Backpropagation wird die Cross-Entropy Loss Function verwendet.
Das Ergebnis ist, dass das trainierte neuronale Netzwerk für das Wört \textit{NLP} lernt, dass das nächste Wort mit der Wahrscheinlichkeit eins das Wort \textit{ist} ist.
Die gelernten Weights bilden einen Vektor, welcher in einer Vektordatenbank gespeichert werden kann.
Das Ergebnis ist eine Vektordatenbank, welche Informationen darüber enthält, welche Wörter semantisch ähnlich sind, und welche nicht.

\subsection{Positional Encoding}
\label{chap:positional-encoding}
Positional Encoding wird eingesetzt, um die Position eines Wortes in einem Satz zu kodieren, wenn ein Satz in einen Vektor transformiert wird.
Das Positional Encoding wird angewendet, nachdem die Wörter mithilfe von Word Embeddings in Vektoren transformiert wurden.
Nun werden die generierten Vektoren mit einem Vektor addiert, welcher die Position des Wortes repräsentiert.
Der Vektor, welcher auf die Word Embeddings addiert wird, wird mithilfe von Sinus- und Kosinusfunktionen berechnet.
Sinus- und Kosinusfunktion sind surjektiv.
Das bedeutet, dass es keine eindeutige Abbildung von einem Inputwert auf einen Output-Wert gibt.
Da nun die Position der Wörter in einem Satz eindeutig kodiert werden sollen, wird für jede Dimension in dem Vektor eine andere Sinus- bzw. Kosinusfunktion verwendet.
Und mit steigender Dimension wird die Frequenz der Sinus- bzw. Kosinusfunktion verringert.
Der Unterschied in der Frequenz hat zur Folge, dass jeder Vektor zur Bestimmung der Position eines Wortes eindeutig ist.

\subsection{Self-Attention}
\label{chap:self-attention}
Der Satz \textit{Software-Entwickler verwenden die Spezifikation, damit sie Features korrekt implementieren} besitzt beispielsweise eine Cross-Referenz.
Das Wort \textit{sie} bezieht sich in dem Beispielsatz auf Software-Entwickler, aber es könnte sich auch auf die Spezifikation beziehen.
Im Allgemeinen können Menschen solche Cross-Referenzen intuitiv verstehen.
Mit der bisher erörterten Architektur von Sentence Transformers, können diese allerdings keine Cross-Referenzen verstehen.
Self-Attention ist eine Technologie, welche es Transformern erlaubt die Cross-Referenz zwischen Wörtern zu verstehen.\\

Um dies zu ermöglichen wird die Ähnlichkeit zwischen Wörtern in einem Satz berechnet.
Für jedes Wort wird also die Ähnlichkeit zu allen anderen Wörtern berechnet, sowie die Ähnlichkeit zu sich selbst.
Um die Ähnlichkeiten zu berechnen, werden auf Basis der Positional Encodings neue Self-Attention Vektoren generiert.
Auf Basis der Positional Encodings werden für jedes Wort in einem Satz eine Query, ein Key und ein Value berechnet.
Die Begriffe Query, Key und Value entstammen hierbei wiederum aus dem Information Retrieval.
Im Information Retrieval ist die Query die gemachte Sucheingabe, der Key eine Kategorie von Dokumenten und der Value ist ein tatsächliches Dokument.
In diesem Kontext wird das Punktprodukt aus der Query und dem Key berechnet und es wird die SoftMax Funktion auf das Ergebnis angewendet.
Das Ergebnis repräsentiert für das aktuelle Wort, für welches der Self-Attention Wert berechnet werden soll, wie stark eine Suche nach diesem Wort, mit einer bestimmten Kategorie von Wörtern (mit einem Konzept) übereinstimmt.
Dann wird wiederum das Punktprodukt auf dieses Ergebnis und den Value jedes Wortes angewendet.
Diese Ergebnisse bedeuten wiederum, wie repräsentativ die einzelnen Wörter für das gegebene Konzept sind.
Die Ergebnisse dieser Operationen sind die Self-Attention Werte der Wörter in einem Satz.

\subsection{seq2seq}
Ein seq2seq-Modell soll eine Sequenz auf eine andere Sequenz anhand von erlernten Regeln mappen.
Dazu wird die Input-Sequenz zunächst in eine modell-interne Sequenz encodiert, der Context Vector, und anschließend in die Output-Sequenz dekodiert.
Wenn eine Wortsequenz in eine andere Wortsequenz umgewandelt werden soll, dann müssen die Input-Wörter zunächst durch ein Word Embedding Layer verarbeitet werden, welches die Wörter in Word Embeddings konvertiert.
Auf das Word Embedding Layer folgen eine beliebige Anzahl von LSTM-Layers.
Die Anzahl der Nodes der LSTM-Layer entspricht der Anzahl der Nodes im Word Embedding Layer.
Zusätzlich kann jede Node in jedem LSTM-Layer mehrere LSTM Netzwerke besitzen.
Die Weights und Biases der LSTM Netzwerke bilden den Output und damit den Context Vector.
Die bisher beschriebene Architektur wird als Encoder bezeichnet.
Die LSTM Netzwerke des Encoders werden im Decoder gespiegelt.
Die Outputs des oberen LSTM Layers des Decoders werden in ein Fully Connected Layer gegeben und durchlaufen eine Softmax Funktion.
Der Output des Decoders sind die Wahrscheinlichkeiten, mit welchen die möglichen Wörter der Ziel-Sequenz dem Input entsprechen. 

Der Context Vector kann in einer Vektordatenbank gespeichert werden, so wie Word Embeddings in einer Vektordatenbank gespeichert werden können.
Das hat zur Folge, dass die Ähnlichkeit von Sätzen genauso wie die Ähnlichkeit von Wörtern ermittelt werden kann.

\section{Vektor-Indizierung}
\label{chap:vektorindizes}
% TODO: Neu schreiben
Neben einer Volltext-Indizierung können Dokumente in Form von Vektoren indiziert werden.
Dazu werden Dokumente zunächst, wie auch bei der Volltext-Indizierung gecrawlt.
Anschließend durchlaufen die Inhalte der Dokumente ein Preprocessing.
Dieses kann je nach Implementierung variieren.
\myRef{Kapitel}{chap:einspielen-der-daten-in-weaviate} beschreibt, wie in der hier aufgeführten Implementierung das Preprocessing durchgeführt wird.
Das Preprocessing hat den Zweck die Daten an das Schema der Datenbank anzupassen und die Qualität der Daten zu erhöhen.
Außerdem sorgt es für eine kürzere Indizierungszeit.\\

Nach dem Preprocessing werden durch einen Transformer für die Inhalte der Dokumente Vektoren berechnet.
Ein Transformer wird mithilfe von Trainingsdaten darauf trainiert, Vektoren für Wörter zu generieren.
Das trainierte Modell wird nach dem Preprocessing durchlaufen.
Zuletzt werden die Daten in einer Vektordatenbank gespeichert.
Eine Vektordatenbank speichert Daten, wie eine dokumentenbasierte Datenbank.
Dort werden nun sowohl die rohen Daten als auch die Vektoren gespeichert, welche von dem Transformer berechnet wurden.
Die Vektoren haben den Vorteil, dass die Daten in der Datenbank nicht linear gespeichert sind.
Sie sind in einem n-dimensionalen Raum gespeichert, mit dessen Hilfe die semantische Nähe zwischen Dokumenten ausgedrückt werden kann.

\section{Hierarchical Navigable Small Worlds (HNSW)}
HNSW ist ein Nearest Neighbor Suchalgorithmus.
Der Algorithmus findet die ähnlichsten Nachbar-Vektoren zu einem gegebenen Input-Vektor.
Um den Algorithmus umzusetzen werden Skip Lists und Navigable Small Worlds verwendet.
Skip Lists sind klassische Linked Lists, welche aus mehreren Schichten besteht.
Die unterste Schicht entspricht der originalen Linked List und beinhaltet alle Daten.
Jedes höhere Layer beinhaltet nur ein Subset der Nodes des darunterliegenden Layers.
Wenn nun eine Suche in der Skip List durchgeführt wird, dann wird das oberste Layer zuerst durchsucht.
Das oberste Layer wird sequenziell durchlaufen.
Es wird bei dem ersten Element des obersten Layers begonnen.
Solange das aktuelle Element kleiner ist als das gesuchte Element, wird das nächste Element untersucht.
Wenn das Element n dem gesuchten Element entspricht, dann wurde das gesuchte Element gefunden und wird zurückgegeben.
Wenn das nächste Element n+1 größer ist als das gesuchte Element, dann wird das aktuelle Element n in dem aktuellen Layer gefunden.
Da dieses Element noch nicht dem gesuchten Element entspricht, wird nun im tieferen Layer weitergesucht, in welchem mehr Elemente vorhanden sind.
Denn das gesuchte Element liegt zwischen dem gefundenen Element n und dem untersuchten Element n+1.
Der gleiche Algorithmus wird rekursiv für jedes Layer durchgeführt, bis das unterste Layer erreicht wurde.
Wurde am Ende des untersten Layers kein Element gefunden, dann konnte das Element in der Skip List nicht gefunden werden (\cite{10.5555/93711}).\\

Navigable Small Worlds basieren auf dem Small World Phänomen aus der Sozialpsychologie, welches von Stanley Milgram formuliert wurde.
Dieses besagt, dass Menschen untereinander so stark vernetzt sind, dass jeder Mensch über nur wenige andere Menschen mit jedem anderen Menschen vernetzt sind (\cite{Milgram_1967}).
Das Konzept lässt sich auf Graphen in der Informatik übertragen.
Das bedeutet, dass in einem stark vernetzten Graphen jeder Datenpunkt mit jedem anderen Datenpunkt über nur wenige Kanten erreichbar ist.
Greedy Routing Algorithmen können von dem Konzept Gebrauch machen, um den Datenpunkt zu finden, dessen Distanz zu einem bestimmten Datenpunkt am geringsten ist (\cite{Malkov_Yashunin_2020}).
Die Konzepte der Skip List und der Navigable Small World können nun kombiniert werden.
Dazu werden anstelle von LinkedLists Graphen in mehreren Layers gespeichert.
Wie bei Skip Lists, beinhaltet der Graph des untersten Layers alle Datenpunkte.
Jedes Layer darüber beinhaltet nur ein Subset des Layers darunter.
Anstatt die Liste des obersten Layers zu durchlaufen, wird der Graph des obersten Layers durchlaufen.
Es wird der Punkt gefunden, welcher am nähesten am gesuchten Punkt ist.
Der gesuchte Punkt ist dabei das Embedding der Sucheingabe.
Dann wird so lange das nächstuntere Layer betrachtet, bis der Punkt gefunden wurde, welcher der Sucheingabe am nähesten ist.\\

Um nun die Distanz zwischen zwei Punkten zu bestimmen wird eine Distanzmetrik benötigt.
Eine mögliche Distanzmetrik für Vektorräume ist die Cosine Similarity.
Eine andere ist L2-squared.
% TODO: Ergänzen

\section{Implementierung der neuen Suchfunktion}
\label{chap:implementierung}
Nachdem erörtert wurde, was eine semantische Suche ist und wie die zugrunde liegende Technologie funktioniert, wird in diesem Kapitel die Implementierung der neuen Suchfunktion dargestellt.
Die Implementierung setzt die Vektordatenbank Weaviate ein, welche eine semantische Suche ermöglicht.
Es wird als nächstes erklärt, wie die Vektordatenbank aufgesetzt wird.
Anschließend wird erläutert, wie die Daten in Weaviate eingespielt werden.

\subsection{Aufsetzen der Vektordatenbank Weaviate}
Für das Aufsetzen von Weaviate werden zwei Komponenten benötigt.
Zum einen die Vektordatenbank selbst.
Zum anderen ein Transformer, welcher Text entgegennimmt, und diese in Vektoren umwandelt, sodass diese in der Datenbank gespeichert werden können.
Um die beiden Komponenten aufzusetzen wird Docker benutzt.
Das entsprechende docker-compose.yml File ist im Anhang zu finden.
Hier werden die beiden Komponenten definiert.
Unter \textit{t2v-transformers} wird der Transformer konfiguriert.
Es wird das Image eines bereits vortrainierten Transformers verwendet.
Unter \textit{weaviate} wird die Datenbank konfiguriert.
Hier wird mithilfe von den Environment-Variablen \textit{$DEFAULT\_VECTORIZER\_MODULE$}, \textit{$ENABLE\_MODULES$} und \textit{$TRANSFORMERS\_INFERENCE\_API$} konfiguriert, welcher Transformer verwendet werden soll.
So wird konfiguriert, dass der Transformer der anderen Komponente benutzt werden soll.\\

Neben den beiden Docker-Containern für die semantische Suche wird ebenfalls eine Spring Boot Anwendung in einem Docker Container eingesetzt.
Die Spring Boot Anwendung verwendet Kotlin und Gradle.
Mithilfe der Dependency \textit{io.weaviate:client:4.0.1} wird die Client API von Weaviate eingesetzt.
So wird nun eine Verbindung zu der Vektordatenbank aufgebaut.
Diese beinhaltet zu diesem Zeitpunkt noch keine Daten.
Bevor die Daten in die Datenbank eingespielt werden, muss das Schema der Daten angegeben werden.
Der entsprechende Code ist ebenfalls im Anhang zu finden.
Es wird eine Klasse \textit{Document} definiert.
Diese Klasse beinhaltet die Properties \textit{documentUrl}, \textit{h1}, \textit{h2} und \textit{p}.
Es werden in dieser Klasse also die URL des Dokuments, sowie die Inhalte aller h1-, h2 und p-Tags gespeichert.
Außerdem wird als Vectorizer \textit{text2vec-transformers} konfiguriert.\\

\subsection{Einspielen der Daten in Weaviate}
\label{chap:einspielen-der-daten-in-weaviate}
Um nun die Daten aus Confluence in Weaviate einzuspielen, werden zuerst die Daten aus Confluence exportiert.
Beim Export von Confluence-Seiten werden HTML-Dateien generiert.
Es werden keine CSS-, JavaScript- oder Bilddateien generiert.
Die HTML-Dateien werden vorverarbeitet, um dem zuvor definierten Schema zu entsprechen.
Es werden zuerst mithilfe eines Abstract-Syntax-Trees alle Inhalte von h1-, h2- und p-Tags herausgefiltert.
Anschließend werden Punctuations und Stopwords entfernt und die Inhalte druchlaufen einen Tokenizer und einen Stemmer.
Punctuations sind Zeichen, wie die folgenden: \textit{.,;:}.
Stopwords sind Wörter, welche für einen Leser notwendig sind, aber für die Verarbeitung durch einen Algorithmus als unwichtig erachtet werden (\cite{Sarica_Luo_2021}).
Beispiele für Stopwords sind \textit{aber, denn, der}.
Ein Tokenizer trennt einen Text in einzelne Wörter auf.
Aus einem Text, wie \textit{das deployment erfolgt durch ein bash-skript} wird also ein Array, welches folgendermaßen aussieht:\\

\(["das", "deployment", "erfolgt", "durch", "ein", "bash", "skript"]\).\\

Ein Stemmer bestimmt den Wortstamm für die einzelnen Wörter auf Basis von Grammatikregeln.
Die Software nutzt den Porter-Stemmer-Algorithmus.
Aus dem Wort \textit{deployment} wird dadurch beispielsweise \textit{deploy}.
Duplikate von Wörtern werden anschließend verworfen.
Alle Inhalte, welche in h1-Tags stehen, werden konkateniert und einem Datenfeld der Vektordatenbank gespeichert.
Das Gleiche gilt für die übrigen Tags.
Nachdem die Inhalte das Preprocessing durchlaufen haben, werden sie in die Datenbank eingespielt.\\

In \myRef{Kapitel}{chap:vektorindizes} wurde bereits von Performancegründen gesprochen, aus denen das Preprocessing durchgeführt wird.
Da nun die einzelnen Schritte des Preprocessings erklärt wurden, kann auch erklärt werden, warum das Preprocessing die Performance beim Indizieren erhöht.
Zuallererst werden viele Wörter gänzlich verworfen, weil sie Stopwords sind.
Durch den Stemmer werden anschließend ähnliche Wörter zusammengruppiert.
Die Wörter \textit{Heizung} und \textit{heizen} stammen beide vom gleichen Wortstamm \textit{heiz}.
Das bedeutet, dass aus zwei verschiedenen Wörtern, welche beide indiziert werden müssen, ein einziges Wort gemacht wird.
Denn am Ende werden Duplikate, wie bereits erwähnt, verworfen.
Die Anzahl der Wörter, welche indiziert werden müssen, wird dadurch reduziert, und damit auch die Last auf dem Transformer, welche die Vektoren für die Wörter berechnen muss.

% \section{Hybride Suche}
% TODO: Erklären

% \section{Die Konfiguration der Suchfunktionen}
% TODO: Auf Benchmark beziehen und erläutern, wie Weaviate konfiguriert war