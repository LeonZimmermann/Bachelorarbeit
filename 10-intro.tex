\chapter{Einleitung}
Für die tägliche Arbeit benötigt ein Softwareentwickler Informationen, welche über den Code, an dem er arbeitet, hinausgehen.
Um an diese Informationen zu kommen, kann der Softwareentwickler eine Person suchen, welche ihm die gewünschte Information geben kann.
Diese Person kann er im Projekt finden, oder über Websites, wie StackOverflow.
Darüber hinaus wird in vielen Projekten eine Wissensdatenbank angelegt, welche Informationen enthält, welche spezifisch für das Projekt sind.
Eine solche Wissensdatenbank ist Confluence.
Sie bietet eine Suchfunktion, welche es dem Softwareentwickler erleichtern soll, die gewünschten Informationen zu finden.
Beispiele für Informationen sind die Spezifikationen oder Dokumentationen eines Teiles der Software, mit welcher der Softwareentwickler gerade arbeitet.
Oder aber auch Best-Practices, Guides, oder Informationen darüber, wie die Software gestartet oder ausgeliefert wird.
Auch Informationen über den Projektplan sind für einen Softwareentwickler von Bedeutung.\\

Aber nicht immer finden Softwareentwickler die gewünschten Informationen mithilfe der Suchfunktion.
Ziel dieser Arbeit soll es sein, mithilfe von Wissen über Suchalgorithmen und Algorithmen aus dem Natural Language Processing zu zeigen, wie sich bestehende Suchfunktionen verbessern lassen.
Software, wie chatGPT\footnote{https://openai.com/blog/chatgpt} zeigt, dass Large Language Models eine valide Möglichkeit zur Information Extraction sind.
Es ist denkbar, dass Suchfunktionen für Softwareentwickler durch Large Language Models verbessert werden können.
Diese Arbeit erörtert, wie die Technologien von Large Language Models verwendet werden können, um eine semantische Suche zu implementieren.
Die implementierte semantische Suche wird mit der bestehenden Confluence-Suche verglichen, um herauszufinden, ob die semantische Suche eine Verbesserung zu der bestehenden Confluence-Suche darstellt.

\section{Vorgehensweise}
Die Gründe, warum eine gewünschte Information schwierig zu finden ist, sind vielzählig.
Manchmal kennt der Softwareentwickler nicht das genaue Wording, um die gewünschten Informationen zu finden.
Manchmal ist das Abstraktionslevel der gefundenen Informationen nicht das, welches sich der Softwareentwickler gewünscht hat.
Beispielsweise, wenn eine allgemeine Definition von Domänenobjekten gesucht wird, aber eine Spezifikation eines Anwendungsfalls gefunden wird, in welchem das Domänenobjekt lediglich erwähnt wird.
Im Rahmen dieser Arbeit wird eine neue Suchfunktion entwickelt, welche eine Verbesserung zu bestehenden Suchfunktionen von Wissensdatenbanken, wie Confluence, darstellen soll.
Nachdem die neue Suchfunktion entwickelt wurde, wird diese mit einer bestehenden Suchfunktion verglichen, um zu untersuchen, ob die neue Suchfunktion besser ist als eine bestehende Suchfunktion.
Dazu werden objektive Metriken zur Messung von Suchfunktionen dargestellt.
Die einzelnen Schritte für die Entwicklung und Evaluierung der neuen Suchfunktion werden im Folgenden dargestellt:
\begin{itemize}
   \item \textbf{Definition von Anwendungsfällen:}
         Es werden zuerst Anwendungsfälle definiert.
         Die Anwendungsfälle beschreiben die konkreten Situationen, in welchen ein Softwareentwickler eine Suche nutzen könnte.
         Das hilft später dabei Suchfunktionen miteinander zu vergleichen.
         Denn anhand der Anwendungsfälle können realistische Sucheingaben definiert werden.
         Diese Sucheingaben können an zwei verschiedene Suchfunktionen übergeben werden.
         Anschließend können die gefundenen Ergebnisse verglichen werden.
         Die genaue Vorgehensweise für diesen Vergleich wird in \myRef{Kapitel}{chap:vergleich-der-suchfunktionen} erklärt.
   \item \textbf{Technologien von Confluence:}
         Als Nächstes wird die Technologie von Confluence dargestellt.
         Das hilft dabei später die Konzeption einer neuen Suchfunktion zu entwickeln und zu begründen, warum die neue Technologie besser sein soll.
         Im weiteren Verlauf der Arbeit wird erläutert, dass für den Vergleich der Suchfunktionen nicht die tatsächliche Confluence-Suchfunktion als Benchmark für die neue Suchfunktion herangezogen wird.
         Stattdessen wird die neue Suchfunktion so entwickelt, dass sie so konfiguriert werden kann, dass sie eine gute Heuristik für die Confluence-Suchfunktion darstellt.
         Dieses Kapitel erfüllt ebenfalls den Zweck eine vergleichbare Suchfunktion entwickeln zu können, welche anhand der objektiven Kriterien gemessen werden kann.
         Auch bei der Auswertung der Ergebnisse der Suchfunktion sind die Inhalte aus diesem Kapitel relevant.
   \item \textbf{Konzeption der neuen Suchfunktion:}
         Nachdem die Technologien von Confluence erörtert wurden, wird in diesem Kapitel die Theorie für die Implementierung einer Suchfunktion erläutert.
         Es wird zuerst erklärt, dass die neue Suchfunktion eine semantische Suchfunktion ist.
         Dann wird erläutert, was eine semantische Suche ist, und warum eine semantische Suche eine Verbesserung gegenüber den Technologien von Confluence darstellen soll.
         Anschließend wird erklärt, welche Technologien für die Implementierung einer semantischen Suche notwendig sind.
         Zuletzt wird die Implementierung der semantischen Suche mithilfe von der Vektordatenbank Weaviate\footnote{https://weaviate.io/} erklärt.
   \item \textbf{Vergleich der Suchfunktionen:}
         In diesem Kapitel werden zuerst Methoden für die Evaluierung von Suchfunktionen herausgearbeitet.
         Damit wird die Frage beantwortet, wann eine Suchfunktion \textit{gut} ist.
         Das hier erläuterte Wissen wird für die Durchführung der Studie benötigt.
         Denn um festzustellen, ob die Implementierung eine Verbesserung darstellt, muss eine Studie durchgeführt werden.
         Aufgrund des Scopes der Arbeit wird nur eine rudimentäre Studie durchgeführt.
         Die Studie vergleicht die Precision-Werte der Confluence-Suchfunktion und der neuen Suchfunktion mithilfe eines generierten Datensatzes von Sucheingaben und erwarteten Ergebnissen.
         Grundlage für die Generierung dieses Datensatzes sind die Anwendungsfälle, welche zuvor definiert wurden.
         Die Ergebnisse werden dargestellt und diskutiert.
         Es wird gezeigt, dass eine semantische Suche für die generierten Daten und den spezifischen Kontext der Datengrundlage, einen schlechten Precision-Wert besitzt. 
         Dann wird dieses Ergebnis interpretiert und es wird der Schluss gezogen, dass Sentence Transformer besser geeignet sind für Open-Domain Datensätze, im Gegensatz zu Closed-Domain Datensätzen.
         Zuletzt wird der Studienaufbau diskutiert.
    \item \textbf{Zusammenfassung und Ausblick:}
         In diesem Kapitel werden die einzelnen Kapitel der Arbeit nochmal zusammenfassend dargestellt.
         Außerdem wird das Ergebnis der Arbeit zusammenfassend dargestellt.
         In \myRef{Kapitel}{chap:ausblick} werden Ansätze erörtert, welche die Suchergebnisse von semantischen Suchen in spezifischen Domänen verbessern sollen.
\end{itemize}

\section{Verwandte Arbeiten}
Es gibt einige Arbeiten, welche die gleichen oder sehr ähnliche Probleme adressieren.
Die verschiedenen Herangehensweisen werden in diesem Kapitel erläutert.
Zuerst wird der Begriff Traceability behandelt, und wie diese genutzt wird, um Suchfunktionen zu verbessern.
Die Herangehensweisen zur Herstellung von Traceability umfassen die Verwendung von Word Embeddings oder auch ein Task-Based Ansatz.
Es folgt eine Übersicht über Arbeiten, welche sich mit Question Answering beschäftigen.
Dann folgt eine Übersicht über Arbeiten, welche sich mit der Evaluierung von Suchfunktionen auseinandersetzen.\\

% Traceability
Traceability bedeutet, dass sich von einer Stelle im Code, auf die entsprechenden Stellen in anderen Artefakten zurückschließen lässt.
Ein Anwendungsfall für eine solche Traceability-Funktionalität ist \textit{Feature Location}, also das Finden der Spezifikation eines Features, wenn nur der Code vorhanden ist.
Analog dazu ist \textit{Bug Localization} ein Anwendungsfall zum Auffinden von Stellen im Code, welche mit einem Bug zusammenhängen.
Zur Herstellung von Traceability zwischen Code und Dokumentation gibt es verschiedene Ansätze.
Haiduc et al. (\citeyear{Haiduc_Bavota_Marcus_Oliveto_DeLucia_Menzies_2013}) schlagen ein System zur Verbesserung von Sucheingaben vor.
Das System verwendet Query Reformulations, um die Traceability herzustellen.
Query Reformulation bedeutet, dass das System den Softwareentwickler bei der Eingabe einer Suchanfrage zur Suche nach den passenden Artefakten unterstützt.
Dazu gibt der Softwareentwickler zunächst eine Suchanfrage ein, und markiert diejenigen Ergebnisse, welche am relevantesten für ihn sind.
Auf Grundlage der gewählten Ergebnisse und mithilfe eines Machine Learning Algorithmus werden nun Vorschläge für eine verbesserte Suchanfrage gemacht.
Dabei gibt es verschiedene Strategien.
Wenn der Softwareentwickler zu Beginn eine sehr lange Suchanfrage eingegeben hat, dann kann das System eine Reduktion der Suchanfrage vorschlagen.
Hat der Softwareentwickler dagegen lediglich einen Suchbegriff angegeben, so kann das System eine Erweiterung der Suchbegriffe vorschlagen.
Dazu greift das System auf Synonyme des eingegebenen Suchbegriffes zurück.\\

Ye et al. (\citeyear{Ye_Shen_Ma_Bunescu_Liu_2016}) beschreiben, wie Word Embeddings dazu verwendet werden können, um Traceability zwischen Code und anderen Softwareentwicklungs-Artefakten herzustellen.
Word Embeddings sind eine Datenstruktur, welche einem Wort einen Vektor in einem n-dimensionalen Raum zuweist.
Anhand dieses Vektors kann die Ähnlichkeit zwischen Wörtern beschrieben werden.
Ähnliche Wörter haben eine geringe Distanz im n-dimensionalen Raum.
Unähnliche Wörter haben eine hohe Distanz.
Der Algorithmus, welcher die Ähnlichkeit der Wörter bestimmt, macht Gebrauch von der Distributional Hypothesis (\cite{Harris_1954}).
Dieser besagt, dass Wörter, welche im gleichen Kontext verwendet werden, eine ähnliche Semantik besitzen.
Hermit wird also die Ähnlichkeit der Wörter bestimmt.
Dieses Verfahren wird nun sowohl auf den Code angewendet als auch auf die Softwareentwicklungs-Artefakte.\\

Antoniol et al. (\citeyear{Antoniol_Canfora_Casazza_DeLucia_2000}) verwenden einen ähnlichen Ansatz, wie Ye et al.
Auch hier werden Word Embeddings verwendet um Softwareentwicklungs-Artefakte gegen den Code zu matchen.
Hier durchlaufen die Artefakte und der Code zwei verschiedene Pipelines.
Die Wörter der Artefakte in natürlicher Sprache werden in lowercase umgewandelt.
Anschließend werden Stoppwörter entfernt.
Zuletzt werden Flexionen entfernt.
Aus dem Code werden zunächst Identifier extrahiert.
Identifier, welche mehrere Wörter unter Verwendung von CamelCase oder snake\_case beinhalten, werden in die einzelnen Wörter aufgeteilt.
Anschließend werden die Identifier auf die gleiche Art und Weise normalisiert, wie die Wörter der Softwareentwicklungs-Artefakte.
Dann erfolgt sowohl für die Identifier als auch für die Wörter aus den Artefakten die Indizierung, also die Umwandlung in Word Embeddings.\\

Treude et al. (\citeyear{Treude_Sicard_Klocke_Robillard_2015}) entwickeln eine Oberfläche, welche die Suche von \textit{Tasks} ermöglicht.
Damit soll ebenfalls Traceability zwischen Code und Dokumentation hergestellt werden.
Dabei ist unter Task eine Operation im Code zu verstehen.
Sie beschreiben einen Task als Verben, welche mit einem direkten Objekt oder einer Präposition in Verbindung stehen.
Die Autoren nennen die Phrasen \textit{get iterator} und \textit{get iterator for collection} als Beispiele.
Die Software analysiert nun die gesamte Dokumentation und extrahiert Tasks.
Die Tasks werden in einen Index geschrieben, sodass der Softwareentwickler nach ihnen suchen kann.\\

% Question Answering
Neben der Traceability zwischen Code und Dokumentation ist das Question Answering ein Ansatz zur Verbesserung von Suchfunktionen.
Suchfunktionen sind Document Retrieval Systeme.
Sie liefern Dokumente, welche zu der Sucheingabe des Nutzers passen.
Document Retrieval Systeme sind eine Unterkategorie von Information Retrieval Systemen.
Information Retrieval Systeme liefern auf Anfrage Informationen an den Nutzer.
Im Fall einer Suchfunktion werden Dokumente geliefert, welche diese Information beinhalten.
Mithilfe der Dokumente ist der Ort, an dem sich die, vom Nutzer gewünschte, Information befindet eingegrenzt.
Nichtsdestotrotz muss der Nutzer aus dieser Eingrenzung die gewünschte Information manuell extrahieren.
Ganz im Gegensatz zu Question Answering Systemen.
Zhang et al. (\citeyear{Zhang_Chen_Xu_Cao_Chen_Cohn_Fang_2023}) untersuchen verschiedene Herangehensweisen zur Implementierung von Open-Domain Question Answering Systemen.
Open-Domain Question Answering Systeme beantworten allgemeine Fragen eines Nutzers, z.B. basierend auf Informationen von Wikipedia.
Closed-Domain Question Answering Systeme beantworten dagegen Fragen im Kontext einer spezifischen Domäne, z.B. basierend auf unternehmensinternen Informationen.\\
% TODO: closed-domain sentence transformer transformer zu verwandten arbeiten hinzufügen (fine-tuned transformers)

% TODO: Informationen lassen sich auch anreichern: NER etc. (NLP Techniken)
% TODO: Informationen lassen sich aufbereiten: RAG

% Messung von Performance
% TODO: Trec und Benchmarks hinzufügen
Für die Messung der Performance einer Suchfunktion gibt es verschiedene Metriken.
Metriken können systemspezifisch sein, nutzerspezifisch.
Systemspezifische Metriken messen die Performance anhand objektiver Kriterien, welche an dem System gemessen werden können.
Nutzerspezifische Metriken messen die Performance dagegen anhand subjektiver Kriterien.
Die Messung erfordert Probanden, welche die Suchfunktion verwenden.
Die Probanden legen die Performance der Suchfunktion anhand der zu betrachtenen subjektiven Kriterien fest.
Clarke und Willet (\citeyear{Clarke_Willett_1997}) messen die Qualität von Suchfunktionen des World Wide Webs anhand des Recalls.
Um dies zu ermöglichen wird ein Datensatz generiert, welcher Sucheingaben beinhaltet, sowie alle relevanten Dokumente für eine Sucheingabe.
Bar-Ilan (\citeyear{Bar-Ilan_2002}) verwendet die gleichen Metriken.
Hier wird darüber hinaus auf die Problematik der Messung des Recalls eingegangen.
Es wird erläutert, dass zur Messung des Recalls a-priori bestimmt werden muss, welche Dokumente als relevant für eine gegebene Sucheingabe erachtet werden sollten.
Gordon und Pathak (\citeyear{Gordon_Pathak_1999}) behaupten, dass die Bestimmung der Relevanz lediglich dem Nutzer mit dem Bedürfnis nach der Information zu überlassen ist.
Voorhees und Harman (\citeyear{Voorhees_Harman_2001}) behaupten, dass die Bestimmung der Relevanz durch ein Experten-Panel durchgeführt werden sollte.
Sirotkin (\citeyear{Sirotkin_2012}) betrachtet verschiedene Ansätze zur Messung der Performance von Suchfunktionen.
Neben den bereits genannten Metriken von Precision und Recall werden andere Metriken zur Messung der Performance einer Suchfunktion genannt, wie Mean Reciprocal Rank und Maximal Marginal Relevance.\\
