\chapter{Theoretischer Hintergrund}

\section{Suchalgorithmen}

In dem Kapitel "Evaluationsmethoden und -Kriterien" wurde bereits beschrieben, dass eine Suche das Qualitätskriterium der Konformität erfüllen sollte.
Das bedeutet, dass eine Suchfunktion die gängigen Arten von Suchanfragen unterstützen muss.
Dieses Kapitel soll die Arten von Suchanfragen vorstellen.
Es ist wichtig diese zu kennen, um das Qualitätskriterium später bei der Implementierung erfüllen zu können.

\subsection{Keyword Search}
Eine Keyword Search durchsucht Dokumente nach der Sucheingabe des Nutzers.
Die Eingabe wird dabei nicht als ganzes betrachtet, sondern jedes Wort einzeln.
Für jedes Keyword werden Dokumente als Ergebnis angezeigt, wenn dieses in dem Dokument vorhanden ist.
Wenn ein Dokument mehrere der Keywords beinhaltet, wird dessen Relevanz höher eingeschätzt als für Dokumente, welche weniger Keywords enthalten.
Dokumente mit höherer Relevanz werden weiter oben in der Ergebnisliste angezeigt.

\subsection{Phrase Search}
Eine Phrase Search ist die Suche nach Textausschnitten in Dokumenten.
Hier werden nicht mehrere Keywords einzeln betrachtet, sondern die gesamte Eingabe in das Suchfeld als eine Einheit.
Es reicht also nicht mehr aus, dass ein Dokument eines der Wörter enthält.
Es muss die gesamte Sucheingabe als ein String enthalten sein.

\subsection{Boolean Search}
Die Boolean Search bietet die Möglichkeit einen boolschen Ausdruck als Sucheingabe zu machen.
Ein Beispiel dafür ist die Sucheingabe \textit{Dokumentation AND Angular}.
Die Sucheingabe bedeutet, dass die Suchmaschine nur Dokumente als Ergebnis darstellen soll, welche beide Keywords Dokumentation und Angular enthalten.
Die Boolean Search kann auch eine Phrase, wie bei der Phrase Search, beinhalten: \textit{"Dokumentation von Software" AND Angular}.
In diesem Beispiel werden nur Dokumente als Ergebnis nur angezeigt, wenn sie den gesamten String \textit{Dokumentation von Software} enthalten, sowie das Keyword \textit{Angular}.
Bei einer Boolean Search können die boolschen Operatoren \textit{AND}, \textit{OR}, \textit{NOT} beliebig kombiniert werden.

\subsection{Field Search}
Eine Field Search sucht Dokumente anhand von Attributen.
Der Nutzer kann diese Attribute auswählen.
Wenn der Nutzer beispielsweise ein Dokument sucht, welches am 01.01.2005 erstellt wurde, dann kann die Eingabe der Suche so aussehen: "erstelldatum: 01.01.2005".
Es können beliebig viele Attribute verwendet werden, um die Suche einzugrenzen.\\

Neben der Verwendung der Attribute für die Suche selbst, können die Attribute komplementär zu einer anderen Art von Suche verwendet werden.
So kann eine Suchfunktion Buttons bereitstellen, über welche Filter festgelegt werden.
Nun kann eine Keyword Search durchgeführt werden, aber die gefundenen Dokumente werden mithilfe der Filter weiter eingeschränkt.

\subsection{Structured Search}
Eine klassische Suchfunktion verwendet Keywords, um relevante Dokumente für eine Sucheingabe zu ermitteln.
Der Vorteil dieser Art von Suche ist, dass die technische Struktur, in der die Daten vorliegen und gespeichert sind, nicht bekannt sein müssen.
Der Nachteil ist auf der anderen Seite, dass die Suchergebnisse unpräzise sein können.
Wenn beispielsweise der Suchbegriff Kamera eingegeben wird, dann werden Kameras als Ergebnisse zurückgegeben.
Soll die Kamera nun bestimmte Eigenschaften besitzen, dann müssen diese Eigenschaften ebenfalls als Keywords angegeben werden.
Nun wird aber nicht die Suche auf Ergebnisse eingegrenzt, bei denen eine Kamera diese bestimmten Eigenschaften besitzt.
Stattdessen werden Suchergebnisse angezeigt, bei denen einige dieser Keywords vorkommen.
Demgegenüber stehen Datenbankabfragen, beispielsweise mithilfe von SQL.
Bei einer Datenbankabfrage können Objekte abgefragt werden, dessen Eigenschaften ganz bestimmte Werte haben.
Die Ergebnisse, die eine solche Abfrage zurückgibt, sind dabei vollkommen genau.
Es werden keine Objekte zurückgegeben, welche diese Kriterien nicht erfüllen.
Voraussetzung für eine solche Suchabfrage ist allerdings, dass die Struktur der Datenbank a priori bekannt ist.
Dem Nutzer muss der Name der Datenbank, der relevanten Tabellen, sowie der relevanten Properties bekannt sein, damit er das passende SQL für die Datenbankabfrage schreiben kann.
Strukturierte Suchen sollen die Vorteile beider Vorgehensweisen kombinieren.
Die Struktur der Daten soll a priori nicht bekannt sein müssen, aber trotzdem sollen die Suchergebnisse vollkommen präzise sein.

\subsection{Lexical Search}
TODO

\subsection{Semantic Search}
Eine Semantische Suche arbeitet nicht anhand von Keywords, sondern anhand von Bedeutungen von Wörtern.
Sie versteht, dass einige Wörter sehr ähnlich sind, so wie "rollout" und "deployment", und dass diese Wörter oft im gleichen Kontext verwendet werden.
Um zu verstehen, welche Wörter kontextuell zusammengehören, werden die Wörter in einem n-dimensionalen Raum positioniert.
Wörter, die sich sehr ähnlich sind, also im gleichen Kontext verwendet werden, haben in diesem n-dimensionalen Raum eine geringe Distanz.
Wörter, die sich eher unähnlich sind, wie "rollout" und "API", haben eine größere Distanz.\\

Um eine semantische Suche zu implementieren, werden die Technologien von Transformern und Vektordatenbanken verwendet.
Ein Transformer bekommt als Input eine große Menge an Text und mappt die einzelnen Wörter auf einen Vektor einer beliebigen Länge.
Der Vektor, der am Ende herauskommt, beschreibt die Position des Wortes in dem n-dimensionalen Raum.
Der Vektor beschreibt gewissermaßen, wie stark ein Wort in eine abstrakte Kategorie einzuordnen ist.
Jeder Wert im Vektor entspricht einer Kategorie.
Mithilfe der Vektoren können verschiedene Wörter hinsichtlich ihrer Ähnlichkeit analysiert werden.
Ähnliche Wörter habe eine große räumliche Nähe, während zwei Wörter, die in vollkommen unterschiedlichen Kontexten verwende werden eine sehr große Distanz im Raum besitzen.
Nehmen wir für ein Beispiel einen dreidimensionalen Raum an.
Die X-Achse ist beschriftet mit dem Wort „Tier“, die Y-Achse ist beschriftet mit dem Wort „Computer“ und die Z-Achse ist beschriftet mit dem Wort „Mensch“.
Nun geben wir einem Transformer das Wort „Katze“, und der Transformer berechnet einen dreidimensionalen Vektor, welcher das Wort „Katze“ im Raum positioniert.
Weil eine Katze ein Tier ist, ist der X-Wert des Vektors eins.
 Der Wert eins bedeutet, dass das Wort vollständig zu dieser Kategorie gehört.
Da eine Katze überhaupt nichts mit einem Computer zu tun hat, ist der Y-Wert des Vektors 0.
Nun ist eine Katze zwar kein Mensch, aber eine Katze ist ein Haustier von Menschen.
Es ist denkbar, dass die Wörter Katze und Mensch oft im gleichen Kontext verwendet werden, sodass der Wert bei 0,3 liegen könnte.
Damit der Transformer einen Vektor berechnen kann, braucht er eine Menge Daten.
Diese Daten erhält er aus vielen Texten.
Werden zwei Wörter oft im gleichen Text genannt oder kommen zwei Wörter in vielen Texten sehr nahe beieinander vor, dann geht der Transformer davon aus, dass die beiden Wörter ähnlich sind, und berechnet ähnliche Vektoren.
Zuvor müssen die Texte allerdings bereitgestellt werden.
Dazu kann beispielsweise das Internet gecrawlt werden.
Die Ergebnisse des Transformers werden in einer Vektordatenbank gespeichert.
Eine Vektordatenbank ist eine Datenbank, welche Vector Embeddings, also ein Objekt als Key und dessen Vektor als Value speichert.
Bei dem Objekt kann es sich um Wörter handeln, dann wird auch von Word Embeddings gesprochen.
Es können aber auch Daten andere Daten, wie Bilder, Videos oder Audio gespeichert werden.
Der Zweck von Vektordatenbanken ist es, Daten nicht einfach linear zu speichern, sondern in einem Raum.
Die Distanz zwischen zwei Einträgen in diesem Raum beschreibt dessen Ähnlichkeit.
Genau diese Informationen machen sich semantische Suchen zu Nutze.

\section{NLP-Algorithmen}

TODO

\section*{Suchmaschinen-Architektur}


Die Architektur von Suchmaschinen besteht aus drei Komponenten.
Aus dem Crawling, der Indexierung und der Suchfunktion selbst.
Das Crawling ist zuständig für das Finden von Websites.
Die Indizierung ist zuständig für das optimale Speichern der Informationen der Websites, und die Suche ist zuständig für das Verstehen der Nutzeranfrage und die Abfrage der relevantesten Informationen aus dem Index, sowie dessen Verarbeitung und Darstellung.
Im Folgenden wird der Begriff Dokument verwendet, um die Dateien zu beschreiben, welche durch einen Crawler gesucht und durch den Index verarbeitet werden.
Unter Dokumenten können hierbei auch eine Website verstanden, welche durch einen Webcrawler durchsucht werden.

\section{Crawling}
Bei der Implementierung eines Systems für eine Suchfunktion benötigt das System zunächst einen Datensatz von Dokumenten, welche überhaupt grundsätzlich über die Suchfunktion gefunden werden können.
Dieser wird mithilfe eines Crawlers aufgebaut.
Ein Crawler ist ein Algorithmus, welcher ein Dokument als Startpunkt bekommt, und anhand dessen neue Dokumente findet.
Der Algorithmus analysiert dazu das Dokument auf Links, welchen der Algorithmus anschließend folgt.
Die neuen Dokumente werden durch den Index verarbeitet und wiederum auf neue Links analysiert.
Dieses Verfahren kann beliebig lange und beliebig rekursiv durchlaufen werden, um den Index zu erweitern.
Neben dem Crawling können Indizes befüllt werden, indem eine Liste von Dokumenten übergeben werden, welche dem Index hinzugefügt werden sollen.

\section{Indizierung}
Bei der Indexierung werden Wörter und Tokens mit Dokumenten assoziiert.
Dazu werden Wörter aus gecrawlten Dokumenten extrahiert und in die Datenbank geschrieben.
Den Dokumenten werden IDs zugeordnet und diese IDs werden den Wörtern zugeordnet.
Dem Index werden nun weitere Informationen hinzugefügt, wie die Wortfrequenz.
Die Wortfrequenz gibt an, wie oft ein Wort in einem Dokument vorkommt.
Es wird auch gespeichert, an welchen Stelle des Dokuments das Wort vorkommt, und auch in wie vielen Dokumenten ein Wort vorkommt.
Diese Art der Indizierung wird auch als invertierter Index bezeichnet, weil den Wörtern bzw.
Tokens die Dokumente zugeordnet werden, und nicht umgekehrt.
Wenn der Nutzer nun ein Keyword in die Suche eingibt, dann sind diese Keywords oft bereits im Index vorhanden, sodass die Dokumente, welche diese Keywords beinhalten einfach dem Index entnommen werden können.
Aufgabe der Suche ist es anschließend die Ergebnisse aufzubereiten.

\subsection*{Volltext-Indizierung}
Bei der Indizierung der Wörter besteht die Problematik, dass gleiche Wörter in unterschiedlichen Formen existieren können.
So stammen „Heizung“ und „heizen“ beide von dem gleichen Wortstamm „heiz“ ab.
Um bei der Indizierung Speicherplatz zu sparen, können Wörter auf diesen Wortstamm reduziert werden, damit sie als ein einziges Wort betrachtet werden können.
Die Bildung des Wortstamms wird auch als Stemming bezeichnet.
Beim Stemming kann es jedoch zu Overstemming und Understemming kommen.
Overstemming bedeutet, dass zwei Wörter, die eigentlich nichts miteinander zu tun haben, also nicht semantisch gleich sind, den gleichen Wortstamm besitzen und als ein Wort betrachtet werden.
Ein Beispiel hierfür sind die Wörter „Wand“ und „wandere“, wie in „ich wandere“.
Beide besitzen den Wortstamm „wand“ und werden entsprechend als ein Wort betrachtet.
Understemming bedeutet, dass zwei Wörter, die eigentlich etwas miteinander zu tun haben, also semantisch gleich sind, nicht den gleichen Wortstamm besitzen und dadurch als zwei verschiedene Wörter betrachtet werden.
Ein Beispiel hierfür sind die Wörter „absorbieren“ und „Absorption“, welche die Wortstämme „absorb“ und „absorp“ besitzen.
Es gibt Techniken zur Vermeidung solcher Probleme, wie der Einsatz vollständiger morphologischer Analysekomponenten.
Hierauf soll aber nicht weiter eingegangen werden.
Zur Implementierung eines Volltext-Index werden Dokumente und Wörter in einer m x n Matrix angeordnet, wobei m die Anzahl der Dokumente ist und n die Anzahl der Wörter.
Die Werte in dieser Matrix werden anhand einer ausgewählten Metrik bestimmt.
Eine oft verwendete Metrik ist das Verhältnis der Wortfrequenz mit der invertierten Dokumentfrequenz.
Die Wortfrequenz gibt dabei an, wie häufig das in dem Dokument vorkommt.
Die Dokumentfrequenz gibt an, in wie vielen Dokumente ein Wort vorkommt.
Das Verhältnis gibt damit an, wie charakteristisch das Wort für ein bestimmtes Dokument ist.
Wenn das Wort charakteristisch für ein Dokument ist, dann kommt es in diesem Dokument häufig vor, aber in anderen Dokumenten nur selten.
Wenn das Wort nicht charakteristisch für das Dokument ist, dann kommt es in anderen Dokumenten genauso häufig oder häufiger vor als in diesem Dokument.
Die Metrik wird auch als tf-idf-Wert bezeichnet.
TODO Rechnung besser verstehen
Wir können uns nun einen m-dimensionalen Raum vorstellen, in dem jedes Dokument eine Dimension darstellt.
Jedes Dokument liegt räumlich auf der Achse der eigenen Dimension.
Wenn der Nutzer nun 
Die Matrix lässt sich konzeptionell so verstehen, dass sie aus n Merkmalsvektoren besteht.
Verwenden wir die eben beschriebene Metrik, dann gibt jeder Merkmalsvektor eine Gewichtung an, wie charakteristisch ein Wort für die verschiedenen m Dokumente ist.
Nun kann der Nutzer Keywords in die Suche eingeben.
Diese Keywords 

\section{Suffix-Trees}
TODO

\section{N-Gramm}
Ein N-Gramm ist das Ergebnis der Spaltung eines Textes in N Fragmente.
Fragmente können dabei Buchstaben, Wörter, Phoneme, Morpheme etc. sein.
N-Gramme können bei der Indizierung verwendet werden und werden auch in Algorithmen von Stemming und Information Retrieval verwendet, außerdem werden durch N-Gramme Rechtschreibprüfungen oder die Suche nach ähnlichen Wörtern ermöglicht.
Besteht ein Text aus einem Fragment, dann wird von einem Monogramm geredet.
Besteht es aus zwei, wird es Bigramm genannt.\\

Um Algorithmen wie eine Rechtschreibprüfung zu ermöglichen, wird neben N-Grammen auch Ähnlichkeitsmaß benötigt, welches beschreibt, wie ähnlich sich zwei Terme sind.
Das Wort Term ist an dieser Stelle synonym mit Wort zu verstehen.
Der Dice-Algorithmus ist ein Ähnlichkeitsmaß für Terme.
Er zerlegt zwei Terme in dessen N-Gramme und bestimmt anhand dieser die Ähnlichkeit der beiden Terme.
Der Dice-Algorithmus lautet wie folgt:\\

TODO Equation\\

Nehmen wir beispielsweise die beiden Terme a=“Wort“ und b=“Wirt“.
Verwenden wir in diesem Beispiel Bigramme, dann ergeben sich für a folgende Bigramme: „\$W“, „Wo“, „or“, „rt“, „t\$“.
Für b ergeben sich: „\$W“, „Wi“, „ir“, „rt“, „t\$“.
Die Schnittmenge der beiden Mengen, also die Bigramme, welche sowohl in a als auch in b vorhanden sind, lauten „\$W“, „rt“, „t\$“.
Es sind also drei Stück.
Das bedeutet der Zähler der Funktion ergibt 2*3 also 6.
Die Anzahl der Bigramme in a und b sind jeweils 5, also ist der Nenner 5+5, also 10.
Dann ist das Ähnlichkeitsmaß der beiden Wörter bei der Verwendung von Bigrammen 6/10 (60\%).
Bei der Verwendung von Trigrammen besitzt a „\$\$W“, „\$Wo“, „Wor“, „ort“, „rt\$“, „t\$\$“ und b „\$\$W“, „\$Wi“, „Wir“, „irt“, „rt\$, „t\$\$“.
Es gibt drei gemeinsame Trigramme.
Das Ergebnis ist (2*3)/(6+6) also 50\%.
An diesem Beispiel lässt sich zeigen, dass sich das Ergebnis des Ähnlichkeitsmaß verändert, wenn ein anderes N-Gramm verwendet wird.

\section{Semantik der Suche verstehen}
Oft sucht ein Software-Entwickler nach einem bestimmten Bereich in der Spezifikation, aber kennt nicht das genaue Wording, welches auf der Seite verwendet wird.
Beispielsweise könnte ein Softwareentwickler sich in ein neues Projekt einarbeiten, und möchte nun verstehen, wie er die bestehende Anwendung überhaupt in einer Testumgebung starten kann.
Dann gibt er in die Sucheingabe der Wissensdatenbank soetwas ein, wie "deployment".
Wenn nun die Seite, welche das Deployment erklärt, aber nicht genau dieses Wort enthält, dann wird die gewünschte Seite durch die Suche nicht gefunden.
Es sei denn, es handelt sich um eine Semantische Suche.
Eine Semantische Suche versteht Zusammenhänge zwischen verschiedenen Wörtern.
Wenn also auf der Seite das Wort rollout verwendet wird, anstelle von deployment, dann versteht eine Semantische Suche, dass diese beiden Begriffe sehr ähnlich zueinander sind, und zeigt auch Suchergebnisse von Seiten an, welche das Wort rollout verwenden, auch wenn nach dem Wort deployment gesucht wurde.

TODO Anforderungen als Stichpunkte zusammenfassen

% Ggf. Teil der Implementierung
\section{Rollenspezifische Suchfilter}
Eine Möglichkeit relevante Dokumente einfacher zu finden ist die Verwendung von Suchfiltern.
Im Kontext einer Suche, welche auf Software-Entwickler zugeschnitten ist, ist es denkbar einen Suchfilter zu verwenden, welcher nach Software-Informationen filtern.
Also die Spezifikation und Dokumentation der Software, sowie How-To-Guides, Best-Practices und Code Konventionen.
Außerdem ist ein Filter für Informationen zur Infrastruktur denkbar. Also Informationen darüber, wie die Software deployt wird, und wohin sie deployt werden kann.
Außerdem Informationen über häufige Probleme beim Deployment.
Dann ist auch ein Filter für Projektmanagement-Informationen sinnvoll.
Dieser kann verwendet werden, um Informationen zum nächsten Software-Release zu finden, über die Teamaufteilung und Verantwortlichkeiten.
TODO Anforderungen als Stichpunkte zusammenfassen

% Ggf. Teil der Implementierung
\section{Information Extraction}
Eine Suchfunktion ist im Allgemeinen ein Information Retrieval System.
Es wird eine Eingabe gemacht, und relevante Dokumente werden identifiziert und dem Nutzer angezeigt.
Neben dem Information Retrieval gibt es Information Extraction.
Information Extraction entnimmt aus den gefundenen Dokumenten genau die Informationen, welche für den Nutzer relevant sind.
Es werden also nicht nur Dokumente gefunden, sondern gleich aufbereitet.
Das ist vor allem dann praktisch, wenn der Nutzer nach einer Information sucht, welche in einem sehr großen Dokument zu finden ist.
In diesem Fall ist es zwar schön, dass das System das Dokument findet, trotzdem muss der Nutzer sich mit der Suche nach der gewünschten Information innerhalb vom Dokument quälen.
Information Extraction löst genau dieses Problem.
Die Inhalte des Dokuments, welche relevant sind, können bei der Darstellung des Suchergebnisses als Kurztext dargestellt werden, um dem Nutzer gleich die gewünschten Informationen zu liefern.\\

TODO Anforderungen als Stichpunkte zusammenfassen

TODO Erläuterung von Named Entity Recognition, Entity Linking, Parser etc. für die Umsetzung solcher Anwendungsfälle\\

Information Extraction ist der Prozess des Extrahierens von Informationen aus Dokumenten.
Dazu müssen zunächst die Dokumente gefunden werden, welche die passenden Informationen enthalten.
Dieser Schritt wird Information Retrieval genannt.
Das Extrahieren von Informationen mit einer Vielzahl von Algorithmen und erfolgt in mehreren Schritten.
Der erste Schritt von Information Extraction ist das Einteilen des Textes in Sätze.
Dieser Schritt wird Sentence Segmentation genannt.
Anschließend werden aus jedem Satz die einzelnen Wörter extrahiert (Word Tokenization) und kategorisiert als Nomen, Verb, Objekt, Pronomen etc. (Part-of-Speech Tagging).
Für das Part-of-Speech Tagging gibt es zwei Implementierungsansätze, rule-based anhand von Grammatikregeln und statistic-based.
Es folgt Syntactic Parsing , also das Generieren eines Syntaxbaumes, welcher den Text repräsentiert.
Zuletzt wird mithilfe von Coreference Resolution identifiziert, welche Stellen im Text sich auf die gleichen Entitäten beziehen.
Ein Beispiel für Coreference Resolution ist, dass ein System versteht, dass mit „Angela Merkel“ und „Merkel“ oder auch „A.
Merkel“ die gleiche Person gemeint ist.
Mithilfe dieser Schritte können Key Phrase Extraction, Named Entity Recognition, Entity Disambiguation, Relation Extraction und Event Extraction implementiert werden.
Key Phrase Extraction ist das Identifizieren von Schlüsselwörtern im Text, welche für diesen Text charakteristisch sind und welche den Text klassifizieren.
Ein Rezept für das Backen von Schokoladenkeksen könnte beispielsweise mithilfe von Schlüsselwörtern, wie „Backen“, „Kekse“, „Schokolade“ beschrieben werden.
Named Entity Recognition ist das Identifizieren von Personen, Orten und Datumsangaben.
Entity Disambiguation ist das Auseinanderhalten von Entitäten, welche die gleichen oder ähnliche Wörter besitzen.
Unter dem englischen Wort „apple“ lässt sich nicht nur ein tatsächlicher Apfel verstehen, sondern es könnte sich auch um das Unternehmen apple handeln.
Welche Bedeutung des Wortes die korrekte ist, erschließt sich aus dem Kontext.
Das ist die Aufgabe von Entity Disambiguation.\\

TODO Image\\

Teilbereiche von Informationsextraktion sind Named Entity Recognition, Named Entity Linking, Relation Extraction, Knowledge Base Reasoning.
Informationsextraktion ist für viele Anwendungsfälle der erste Schritt in einer größeren Pipeline.
Das könnte z.B. ein Chatbot sein oder ein Übersetzer.
Named Entity Recognition ist die Erkennung von Eigennamen.
Named Entity Linking ist der Prozess, eine bestimmte Entität eine einzigartige Identität zu geben.
Die Entität wird auf eine Target Knowledge Base gemappt, welche weitere Informationen über ebendiese Entität enthält.
Relation Extraction erkennt Beziehung zwischen Entities.
Die Beziehung kann durch ein Subjekt Prädikat Objekt Triple repräsentiert werden.
Abgrenzung von Information Extraction und Information Retrieval.
Information Retrieval selektiert relevante Dokumente.
Information Extraktion extrahiert Informationen aus einem gegebenen Dokument.
Die Klassifikation von Texten wird auch als Topic Modeling bezeichnet.
Es wird die Häufigkeit von Wörtern aus bestimmten Kategorien bestimmt und damit ein Thema zugeordnet.
Dieses Verfahren könnte zur Ermittlung von Labels verwendet werden.
Die Labels gehen dabei über die Inhalte des Textes hinaus.
Ein Algorithmus zum Topic Modeling ist Explicit Semantic Analysis (ESA).
Language Modelling ist ein Task von NLP, welcher es ermöglicht Texte zu übersetzen, Spellchecking, Spracherkennungen und Handschrifterkennung zu implementieren.\\

TODO Image

\section{Feature Location}

Um den Use-Case Feature Location zu implementieren kann ein gemeinsamer Index zwischen Code und Wissensdatenbanken verwendet werden.
Diese werden mithilfe von Word Embeddings in einer Vektordatenbank gespeichert.
Das hat den Vorteil, dass die Ähnlichkeit zwischen Dokumenten in der Wissensdatenbanken und Dateien, Klassen oder Methoden im Code bestimmt werden kann.
Wenn sich der Softwareentwickler also an einer bestimmten Stelle im Code befindet, und zu dieser Stelle weitere Informationen braucht, dann sorgt der gemeinsame Index dafür, dass Dokumente mit großer Ähnlichkeit in der Wissensdatenbank gefunden werden.
Der Softwareentwickler bekommt also die relevantesten Dokumente zu der Stelle Code, die er sich gerade ansieht.
Auf Seiten des Codes wird der Index anhand von Identifiers erstellt \cite{Marcus_Maletic_2003}.

// TODO Quellen, Implementierungen finden (gibt es sowas schon?)