\chapter{Vergleich der Suchfunktionen}
\label{chap:vergleich-der-suchfunktionen}

Zum Vergleich der beiden Suchfunktionen wird im Folgenden ein fiktionales Szenario dargestellt.
Das Szenario beschreibt den Ablauf des Onboardings eines neuen Mitarbeiters.
Dabei werden die expliziten Fragen beschrieben, welche sich der Mitarbeiter stellt, um sich einzuarbeiten.
Es wird angenommen, dass der Mitarbeiter sich mithilfe der Wissensdatenbank einarbeitet und die Suchfunktion der Wissensdatenbank verwendet.
Die Fragen, welche sich der Mitarbeiter des fiktionalen Szenarios stellt, werden anschließend bei der Durchführung einer Studie verwendet.
Die Studie verwendet die Fragen, um die Performance zwischen Suchfunktionen zu vergleichen.
Der genaue Aufbau der Studie ist im Folgenden näher beschrieben.
Nachdem der Aufbau der Studie erklärt wurde, werden die Daten der Studie ausgewertet und dargestellt.
Das Ergebnis wird diskutiert.
Zuletzt wird auf die Validität der Daten eingegangen und es wird der Versuchsaufbau diskutiert.

\section{Aufbau der Studie}
Im Rahmen der Bachelorarbeit wird eine Studie durchgeführt, welche die neue Suchfunktion mit der bestehenden Confluence-Suche vergleicht.
Da der Zeitrahmen begrenzt ist, in welchem die neue Suchfunktion entwickelt wird, wird nicht die tatsächliche Suchfunktion von Confluence entwickelt.
Es wurde bereits erwähnt, dass die Suche von Confluence auf Apache Lucene basiert, und dass dieses ein VSM auf Basis von BM25 verwendet.
Daher wird die neu entwickelte Suchfunktion in mehreren Konfigurationen verglichen.
So wird die neue Suchfunktion so konfiguriert, dass diese eine reine BM25 Suche durchführt.
Diese Konfiguration soll als Benchmark dienen und die tatsächliche Confluence-Suche repräsentieren.
Eine weitere Konfiguration verwendet eine Mischung aus einer BM25 Suche, und einer semantischen Suche.
Die beiden Suchalgorithmen werden zu gleichen Teilen verwendet.
Zuletzt verwendet eine andere Konfiguration lediglich die semantische Suche auf Grundlage von Sentence Similarity.\\

Die Eigenschaft, welche untersucht werden soll ist, ob eine semantische Suche für den gegebenen Datensatz, und im Kontext einer Wissensdatenbank in der Softwareentwicklung, ebenfalls bessere Suchergebnisse liefert, als eine Suche auf Basis von BM25.
Durch die Verwendung einer einheitlichen Implementierung wird sichergestellt, dass lediglich die Unterschiede der Suchalgorithmen untersucht werden.
Es wird damit verhindert, dass die Confluence-Suche besser abschneidet, weil sie ausgereifter ist.
Es wird ebenfalls sichergestellt, dass die Datensätze der Suchfunktionen identisch sind.\\

Für die Studie wurde ein Datensatz generiert, welcher Dokumente beinhaltet, welche durch die Suchfunktion gefunden werden sollen.
Der Datensatz wurde aus einem realen Softwareentwicklungs-Projekt generiert, indem ein Teil der tatsächlichen Confluence-Seiten exportiert wurde.
Für jedes Dokument sind eine oder mehrere Sucheingaben definiert, mit dessen Eingabe das Dokument gefunden werden soll.
Darüber hinaus ist für jedes Dokument festgehalten, zu welchem Anwendungsfall sich dieses zuordnen lässt.
\myRef{Tabelle}{ausschnitt-studie-ergebnisse} zeigt einen Ausschnitt aus den Daten, welche durch die Studie generiert wurde.
Der Name des Projektes, aus welchem die Daten entnommen wurden, wurde unkenntlich gemacht.\\

\begin{table}[!ht]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
        Anwendungsfall & Bereich & Sucheingabe & Erwartetes Dokument \\ \hline
        Onboarding & Entwicklung & Getting Started & Getting Started \\ \hline
    \end{tabular}}
\end{table}

\begin{table}[!ht]
    \centering
    \label{ausschnitt-studie-ergebnisse}
    \resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
        Gefundene Dokumente & Hit & Hit in Five & Hit in Three &  Hit in One \\ \hline
        getting started (legacy)..., onboarding usecase x..., onboarding re..., ui-debian-build-base..., ep... & true & true & true & true
    \end{tabular}}
    \caption[Ausschnitt der Ergebnisse]{Ausschnitt aus den Ergebnissen der Studie}
\end{table}

Die Suchfunktion gibt für jeden Algorithmus fünf Dokumente als Antwort auf eine Sucheingabe zurück.
Diese fünf Dokumente werden nach dessen Score sortiert.
Das bedeutet, dass das erste Dokument der Liste jenes ist, welches von dem Algorithmus als das passendste erachtet wird.
Die fünf Dokumente werden darauf untersucht, ob sich das gewünschte Dokument unter den Dokumente befindet.
Das Ergebnis ist ein Precision-Score für den Suchalgorithmus.
Um eine detailliertere Analyse zu ermöglichen wird nicht nur die Precision in Bezug auf die ersten fünf Dokumente gemessen.
Es wird die Precision für das erste Dokument (Hit in One), die ersten drei Dokumente (Hit in Three) und alle fünf Dokumente (Hit in Five / Hit) gemessen.
Anschließend werden die Precision-Scores der Algorithmen miteinander verglichen.
Dazu wird die Summe der Hits für alle Sucheingaben gebildet.
Die Summe wird durch die gesamte Anzahl der Sucheingaben dividiert.\\

Die Studie wird vollkommen automatisch durchgeführt.
Dadurch können Ergebnisse der Studie nicht durch Teilnehmer verfälscht werden.
Es bedeutet auch, dass die Studie eine hohe Reliabilität hat und mit jeder Durchführung das gleiche Ergebnis liefert.
Sie ist also reproduzierbar.
Der Nachteil dieser Herangehensweise ist, dass die subjektive Wahrnehmung des Nutzers, in Bezug auf die Precision der Suchfunktion, nicht beachtet werden kann.
So ist es denkbar, dass eine Sucheingabe nicht das gewünschte Dokument beinhaltet, aber andere Dokumente, welche ein Nutzer als sinnvoll erachten würde.
Die Ergebnisse der Studie sind damit abhängig von der Vorauswahl der Dokumente, welche gefunden werden sollen, und der Sucheingaben, welche a priori als sinnvoll bestimmt wurden.
Es ist möglich, dass eine Suchfunktion für eine Sucheingabe durchaus ein sinnvolles Ergebnis liefert, aber nicht das Ergebnis, welches durch den Aufbau der Studie erwartet wird.

\section{Auswertung der Ergebnisse}
Die Studie wurde mehrfach durchgeführt.
Die Precision-Scores werden auf zwei Nachkommastellen gerundet.
Die Precision-Scores werden in der \myRef{Tabelle}{precision-tabelle} zusammengefasst.

\begin{table}[!ht]
    \centering
    \label{precision-tabelle}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Precision & Hit in Five & Hit in Three & Hit in One \\ \hline
        BM25 & 0,56 & 0,5 & 0,39 \\ \hline
        Hybrid & 0,56 & 0,52 & 0,35 \\ \hline
        Semantic & 0,15 & 0,13 & 0,07 \\ \hline
    \end{tabular}
    \caption[Precision]{Precision der Suchfunktionen}
\end{table}

Es ist auffällig, dass der Precision-Score der semantischen Suche wesentlich schlechter ist als der Precision-Score der BM25 Suche.
Dies könnte sich wie folgt erklären lassen.
Bei der Implementierung der Suche wurden alle H1-Header, H2-Header und Paragraphen eines Dokuments zusammengefasst in jeweils einen String für das entsprechende HTML-Tag.
Der Algorithmus der Suchfunktion vergleicht nun den String der Sucheingabe mit den Strings in den Dokumenten.
Also mit dem String des Titels, dem String der H1-Header, dem String der H2-Header und dem String der Paragraphen.
Weil die einzelnen Inhalte der HTML-Tags zusammengefasst werden, können diese Strings größer werden als die Sucheingabe, welche in der Regel zwei bis drei Wörter umfasst.
Dadurch, dass die Sätze einen großen Größenunterschied haben, fällt entsprechend auch der Score der Dokumente niedrig aus.
Da nun alle Scores niedrig sind kann die Suchfunktion nicht so leicht das passendste Dokumente finden.
Dadurch werden zum Teil weniger relevante Dokumente gefunden und der Precision-Score fällt besonders niedrig aus.\\

Auch das Postprocessing könnte eine Auswirkung auf den Precision-Score der semantischen Suche haben.
Für Bag of Words Modelle, wie BM25, ist es typisch ein Stemming vor der Indizierung durchzuführen.
Zum einen hat dies positive Auswirkungen auf den Precision-Score.
Zum sorgt dies dafür, dass verschiedene Tokens als das gleiche Wort betrachtet werden, auch wenn sie nicht in der gleichen morphologischen Form stehen.
Die Details zu dieser Problematik wurden bereits in Kapitel \ref{chap:volltext-indizierung} erläutert.
Für die semantische Suche könnte ein Stemming allerdings negative Auswirkungen haben.
Denn bei dem Training eines Sentence Transformers werden ganze Sätze betrachtet und nicht nur die Stammformen der einzelnen Wörter.
Das bedeutet, dass der Sentence Transformer die Wörter nicht mehr korrekt semantisch zuordnen kann.
Dies wirkt sich wiederum negativ auf den Precision-Score der semantischen Suche aus.\\

Die Hypothese ist also, dass der Precision-Score der semantischen Suche nur aus dem Grund gering ist, weil neben dem Title-Tag auch die weiteren Tags indiziert werden.
Um diese Hypothese zu überprüfen wurde die Studie ein weiteres Mal durchgeführt.
Dieses Mal wurden lediglich die Title-Tags indiziert.
Das Title-Tag ist für jedes Dokument nur einmal vorhanden.
Außerdem sind die Titelsätze der Dokumente wesentlich kürzer als ganze Paragraphen.
Mit dieser Konfiguration sollte der Größenunterschied des Sucheingabe-Strings und der Titel-Strings kein entscheidender Faktor mehr sein.
Das Ergebnis dieser Konfiguration ist in \myRef{Tabelle}{precision-tabelle-2} dargestellt.\\

\begin{table}[!ht]
    \centering
    \label{precision-tabelle-2}
    \begin{tabular}{|l|l|l|l|}
        \hline
        Precision & Hit in Five & Hit in Three & Hit in One \\ \hline
        BM25 & 0,54 & 0,5 & 0,39 \\ \hline
        Hybrid & 0,54 & 0,48 & 0,35 \\ \hline
        Semantic & 0,28 & 0,22 & 0,11 \\ \hline
    \end{tabular}
    \caption[Precision (2. Durchführung)]{Precision der Suchfunktionen (2. Durchführung der Studie)}
\end{table}

In dieser Konfiguration hat die semantische Suche eine bessere Precision.
Die Hit in Five Precision liegt bei 0,28 für die Konfiguration, welche nur den Titel der Dokumente indiziert, gegenüber 0,15 bei einer vollständigen Indizierung.
Die Verbesserung der Precision kann mit drei Faktoren begründet werden.
Zum einen die beiden Hypothesen, welche mit dieser Beobachtung untersucht werden sollten.
Also zum einen der Größenunterschied von Sucheingabe-String und den Strings, welche indiziert werden.
Zum anderen die Tatsache, dass sich das Postprocessing der Dokumente negativ auf die Fähigkeit des Sentence Transformers auswirkt, die Semantik der Wörter zu verstehen.
Außerdem besteht die Möglichkeit, dass die Inhalte der Dokumente leicht von den Titeln der Dokumente abweichen können.
Das würde bedeuten, dass für den gegebenen Versuchsaufbau der Titel besser geeignet ist, um die gewünschten Dokumente zu finden, als das gesamte Dokument.
Die Precision für die BM25-Suche ist bei vollständig indizierten Dokumenten besser als bei alleiniger Beachtung des Titels.
Lediglich die semantische Suche hat sich bei der zweiten Konfiguration verbessert.
Das bedeutet, dass sich der gezeigte Effekt der Verbesserung der Suchfunktion tatsächlich mit der Hypothese begründen lässt, dass die Länge der Strings ein zu berücksichtigender Faktor ist.
Da die semantische Suche weiterhin schlechter ist als die BM25-Suche, scheint es aber weitere Faktoren zu geben, welche zu berücksichtigen sind.\\

% TODO: Weaviate Benchmarks diskutieren

Eine mögliche Erklärung für die weiterhin schlechten Ergebnisse der semantischen Suche ist die Tatsache, dass es sich bei dem verwendeten Datensatz um eine Closed-Domain handelt.
Das bedeutet, dass der Textcorpus Fachbegriffe beinhaltet, welche der Transformer mit hoher Wahrscheinlichkeit nicht kennt.
Dementsprechend schwer fällt es dem Transformer folglich, passende Vektoren zu generieren, welche die Semantik von den domänenspezifischen Fachbegriffen abbilden.
Die Paper \textit{GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval} (\cite{Wang_Thakur_Reimers_Gurevych_2022} von Wang et. al. und \textit{TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning}\cite{Wang_Reimers_Gurevych_2021}) von Wang et. al. untersuchen Möglichkeiten, um Sentence Transformer an eine Domäne anzupassen.
Dieser Vorgang wird als Domain Adaption bezeichnet.

\section{Diskussion des Studienaufbaus}
Wie bereits beschrieben gibt der Recall an, wie viele der relevanten Dokumente gefunden wurden.
Die Precision gibt lediglich an, wie viele der Dokumente, welche gefunden wurden, relevant sind.
Eine optimale Suchfunktion würde per Definition alle Dokumente finden, welche relevant sind, und keine irrelevanten Dokumente.
Umgekehrt ist eine schlechte Suchfunktion eine Suchfunktion, welche keine relevanten Dokumente findet, sondern nur irrelevante.
Dabei spielt es für den Nutzer aber keine Rolle, ob irrelevante Dokumente gefunden wurden.
Die Tatsache, dass die relevanten Dokumente nicht gefunden wurden, machen die Suchfunktion für den Nutzer nicht benutzbar. 
Die Studie untersucht die Precision der Suchalgorithmen.
Auf Grundlage der obigen Argumentation zeigt sich, dass der Recall-Score wichtiger ist als der Precision-Score.
Denn ist der Recall gering, dann ist die Suchfunktion nicht benutzbar.
Eine Suchfunktion mit einem hohen Recall, aber eine niedrigen Precision könnte dagegen viele relevanten Dokumente finden, aber auch viele irrelevante Dokumente.
Solange die relevanten Dokumente in der Liste weiter oben dargestellt werden, ist diese Zusammensetzung aus Precision und Recall gut.\\

Die Studie hat die Precision für das erste, die ersten drei und alle fünf Dokumente gemessen.
Die Studie hat allerdings keinen Recall gemessen, obwohl es sinnvoll ist den Recall als den wichtigeren Score einer Suchfunktion zu erachten.
% TODO: Warum ist der Recall wichtiger?
Grund dafür ist die Tatsache, dass um den Recall messen zu können, alle relevanten Dokumente für eine Sucheingabe bekannt sein müssen.
Um für eine Studie a priori Sucheingaben zu bestimmen, und alle Dokumente, welche für diese Sucheingabe relevant sind, müsste der gesamte Datensatz bekannt sein.
Da der Datensatz mehrere hundert Dokumente beinhaltet ist dies nicht möglich.
Und auch wenn der gesamte Datensatz bekannt wäre, dann müsste trotzdem eine Entscheidung darüber getroffen werden, welche Dokumente relevant sind und welche nicht.
Das wäre wiederum eine subjektive Entscheidung, sodass die Validität des Ergebnisses anzweifelbar wäre.

% TODO: Question Answering (Frage: was sind die dateiformate für den auftragseingang?/was für dateien gebe ich in den auftragseingang?, Antwort: excel - dateien oder csv - dateien)