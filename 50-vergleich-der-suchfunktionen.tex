\chapter{Vergleich der Suchfunktionen}
\label{chap:vergleich-der-suchfunktionen}
%TODO: Kapitel erweitern
%TODO: Testdaten äquivalente Beispiele darstellen
Zum Vergleich der beiden Suchfunktionen wird im Folgenden zuerst auf Evaluationsmethoden und -Kriterien eingegangen.
Diese sind die Grundlage für den Vergleich von zwei Suchfunktionen.
Anschließend wird der Aufbau der Studie dargestellt, sowie der zugrundeliegende Datensatz für die Durchführung der Studie.
Nachdem der Aufbau der Studie erklärt wurde, werden die Daten der Studie ausgewertet und dargestellt.
Das Ergebnis wird diskutiert.
Zuletzt wird auf die Validität der Daten eingegangen und es wird der Versuchsaufbau diskutiert.

\section{Evaluationsmethoden und -Kriterien}
\label{chap:evaluationsmethoden}
Um zu verifizieren, dass die Implementierung der neuen Suchfunktion ihre gewünschte Wirkung erzielt, müssen zuerst Methoden und Kriterien zur Messung herangezogen werden.
Karl Popper gilt als Begründer des kritischen Rationalismus.
Es war der Anfang von wissenschaftlich durchgeführten Experimenten.
Bei einem Experiment wird zunächst eine Hypothese aufgestellt.
Diese wird anschließend durch das Experiment untersucht.
Ein solches Experiment wird für die Evaluation der Suchfunktionen erstellt.
Diese stützt sich auf den Anwendungsfällen, welche in \myRef{Kapitel}{chap:anwendungsfaelle} dargestellt wurden.
Dazu werden für die Anwendungsfälle Sucheingaben erstellt.
Für jede Sucheingabe wird definiert, welche Dokumente als Ergebnis erwartet werden.
Die Hypothese: Mit der implementierten Suchfunktion werden die erwarteten Dokumente \textit{besser} gefunden als mit der bisherigen Suchfunktion.
Diese Hypothese muss nun quantifizierbar und messbar gemacht werden.
Die subjektive Wahrnehmung reicht nicht für eine wissenschaftliche Arbeit aus.\\

Die Formulierung der Hypothese ist für sich genommen zu unspezifisch.
Es muss geklärt werden, wann eine Information \textit{besser} zu finden ist.
Im Folgenden werden Precision, Recall und F-Maß herangezogen, um Suchfunktionen anhand dieser Eigenschaften vergleichbar zu machen.
Mithilfe dieser Messwerte kann eine objektive Entscheidung darüber getroffen werden, welche Suchfunktion \textit{besser} funktioniert.

\subsection{Precision, Recall und F-Maß}
Zur Evaluation der Suchfunktionen werden später die statistischen Messwerte Precision und Recall verwendet.
Die Messwerte beschreiben, inwieweit eine Hypothese zutrifft.
Wenn also die Hypothese ist, dass ein bestimmtes Dokument gefunden wird, dann ist der Precision-Wert das Verhältnis zwischen allen gefundenen Dokumenten und den gefundenen Dokumenten, die tatsächlich relevant sind.
Der Wert lässt sich im Kontext der Suche nach Dokumenten wie folgt definieren (\cite{Sirotkin_2012}):\\

\(Precision=P=\frac{gefundene relevante Dokumente}{gesamte Anzahl gefundener Dokumente} \)\\

Der Recall-Wert gibt wiederum an, wie viele von den tatsächlich relevanten Dokumenten auch gefunden wurden.
Er lässt sich in diesem Kontext wie folgt definieren (\cite{Sirotkin_2012}):\\

\(Recall=R=\frac{gefundene relevante Dokumente}{gesamte Anzahl relevanter Dokumente}\)\\

Es ist schwierig beide Werte zu optimieren, da der Precision-Wert versucht die Anzahl der gefundenen Dokumente einzugrenzen und der Recall-Wert versucht die Anzahl der gefundenen Dokumente zu erweitern.
Das F1-Maß fasst beide Werte zu einem neuen Wert zusammen (\cite{Sirotkin_2012}):\\

\(F_1=2\frac{PR}{P+R}\)\\

Neben der Verwendung des F1-Maß ist es wichtig sich Gedanken darüber zu machen, welcher der beiden Messwerte wichtiger ist.
In diesem Fall ist es sinnvoll eher die Precision zu optimieren.
Denn, wenn ein Dokument gesucht wird, aber überhaupt nicht gefunden werden kann, dann erfüllt die Suchfunktion nicht ihren Zweck.
Wenn die Suchfunktion irrelevante Dokumente darstellt, kann sie trotzdem ihren Zweck erfüllen, solange die relevantesten Dokumente zuerst in der Liste der Ergebnisse dargestellt wird.
Dieser Faktor gilt auch bei der Implementierung zu berücksichtigen.

\section{Aufbau der Studie}
\label{chap:aufbau-der-studie}
Im Rahmen der Bachelorarbeit wird eine Studie durchgeführt, welche die neue Suchfunktion mit der bestehenden Confluence-Suche vergleicht.
Da der Zeitrahmen begrenzt ist, in welchem die neue Suchfunktion entwickelt wird, wird nicht die tatsächliche Suchfunktion von Confluence als Benchmark für die neue Suchfunktion herangezogen.
Das ist notwendig um einen gute Vergleichbarkeit zwischen den Suchfunktionen zu gewährleisten.
Die Produktreife von Confluence könnte die Performance dessen Suchfunktion positiv beeinflussen.
Da die neue Suchfunktion in nur kurzer Zeit entwickelt wurde, kann der Unterschied Produktreife ein Störfaktor bei dem Vergleich der Suchfunktionen sein.
Um dies zu vermeiden wird die Confluence-Suche repräsentiert durch eine BM25 Suche, welche ebenfalls mit Weaviate implementiert wurde. 
Es wurde bereits erwähnt, dass die Suche von Confluence auf Apache Lucene basiert, und dass dieses ein VSM auf Basis von TF-IDF verwendet.
Sparck et. al. beschreiben, dass eine große Ähnlichkeit bei der Performance zwischen TF-IDF und BM25 besteht (\cite{Sparck_Jones_Walker_Robertson_2000}).
Auf diese Weise wird die Vergleichbarkeit zwischen den beiden Suchfunktionen also verbessert.
Die BM25-Suche in Weaviate ist eine von drei Konfigurationen der neuen Suchfunktionen.
Sie ist die Benchmark für die neue Suchfunktion und repräsentiert die Confluence-Suche.

Eine weitere Konfiguration verwendet eine Mischung aus einer BM25 Suche, und einer semantischen Suche.
Die beiden Suchalgorithmen werden zu gleichen Teilen verwendet.
Zuletzt verwendet eine andere Konfiguration lediglich die semantische Suche auf Grundlage von Sentence Similarity.
Die Eigenschaft, welche untersucht werden soll ist, ob eine semantische Suche für den gegebenen Datensatz, und im Kontext einer Wissensdatenbank in der Softwareentwicklung, ebenfalls bessere Suchergebnisse liefert, als eine Suche auf Basis von BM25.
Durch die Verwendung einer einheitlichen Implementierung wird sichergestellt, dass lediglich die Unterschiede der Suchalgorithmen untersucht werden.
Es wird damit verhindert, dass die Confluence-Suche besser abschneidet, weil sie ausgereifter ist.
Es wird ebenfalls sichergestellt, dass die Datensätze der Suchfunktionen identisch sind.\\

Für die Studie wurde ein Datensatz generiert, welcher Dokumente beinhaltet, welche durch die Suchfunktion gefunden werden sollen.
Der Datensatz wurde aus einem realen Softwareentwicklungs-Projekt generiert, indem ein Teil der tatsächlichen Confluence-Seiten exportiert wurde.
Die Software soll im Gesundheitswesen eingesetzt werden.
Das bedeutet, dass die Wissensdatenbank Fachbegriffe aus dem Gesundheitswesen beinhaltet.
Darüber hinaus beinhaltet die Wissensdatenbank Definitionen, welche sich spezifisch auf das Softwareprojekt beziehen.
Daher handelt es sich bei dem Datensatz um eine Closed-Domain, also ein Datensatz, welcher spezifisch aus einer bestimmten Domäne generiert wurde.
Das steht im Gegensatz zu einem Open-Domain Datensatz, welcher Daten beinhaltet, welche allgemein gültig sind, und nicht nur in einer bestimmte Domäne.
Für jedes Dokument sind eine oder mehrere Sucheingaben definiert, mit dessen Eingabe das Dokument gefunden werden soll.
Darüber hinaus ist für jedes Dokument festgehalten, zu welchem Anwendungsfall sich dieses zuordnen lässt.
\myRef{Tabelle}{ausschnitt-studie-ergebnisse} zeigt einen Ausschnitt aus den Daten, welche durch die Studie generiert wurde.
Der Name des Projektes, aus welchem die Daten entnommen wurden, wurde unkenntlich gemacht.\\

\begin{table}[!ht]
    \centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
        Anwendungsfall & Bereich & Sucheingabe & Erwartetes Dokument \\ \hline
        Onboarding & Entwicklung & Getting Started & Getting Started \\ \hline
    \end{tabular}}
\end{table}

\begin{table}[!ht]
    \centering
    \label{ausschnitt-studie-ergebnisse}
    \resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
        Gefundene Dokumente & Hit & Hit in Five & Hit in Three &  Hit in One \\ \hline
        getting started (legacy)..., onboarding usecase x..., onboarding re..., ui-debian-build-base..., ep... & true & true & true & true
    \end{tabular}}
    \caption[Ausschnitt der Ergebnisse]{Ausschnitt aus den Ergebnissen der Studie}
\end{table}

Die Suchfunktion gibt für jeden Algorithmus fünf Dokumente als Antwort auf eine Sucheingabe zurück.
Diese fünf Dokumente werden nach dessen Score sortiert.
Das bedeutet, dass das erste Dokument der Liste jenes ist, welches von dem Algorithmus als das passendste erachtet wird.
Die fünf Dokumente werden darauf untersucht, ob sich das gewünschte Dokument unter den Dokumente befindet.
Das Ergebnis ist ein Precision-Score für den Suchalgorithmus.
Um eine detailliertere Analyse zu ermöglichen wird nicht nur die Precision in Bezug auf die ersten fünf Dokumente gemessen.
Es wird die Precision für das erste Dokument (Hit in One), die ersten drei Dokumente (Hit in Three) und alle fünf Dokumente (Hit in Five / Hit) gemessen.
Anschließend werden die Precision-Scores der Algorithmen miteinander verglichen.
Dazu wird die Summe der Hits für alle Sucheingaben gebildet.
Die Summe wird durch die gesamte Anzahl der Sucheingaben dividiert.\\

Die Studie wird vollkommen automatisch durchgeführt.
Dadurch können Ergebnisse der Studie nicht durch Teilnehmer verfälscht werden.
Es bedeutet auch, dass die Studie eine hohe Reliabilität hat und mit jeder Durchführung das gleiche Ergebnis liefert.
Sie ist also reproduzierbar.
Der Nachteil dieser Herangehensweise ist, dass die subjektive Wahrnehmung des Nutzers, in Bezug auf die Precision der Suchfunktion, nicht beachtet werden kann.
So ist es denkbar, dass eine Sucheingabe nicht das gewünschte Dokument beinhaltet, aber andere Dokumente, welche ein Nutzer als sinnvoll erachten würde.
Die Ergebnisse der Studie sind damit abhängig von der Vorauswahl der Dokumente, welche gefunden werden sollen, und der Sucheingaben, welche a priori als sinnvoll bestimmt wurden.
Es ist möglich, dass eine Suchfunktion für eine Sucheingabe durchaus ein sinnvolles Ergebnis liefert, aber nicht das Ergebnis, welches durch den Aufbau der Studie erwartet wird.

\section{Auswertung der Ergebnisse}
Die Studie wurde mehrfach durchgeführt.
Die Precision-Scores werden auf zwei Nachkommastellen gerundet.
Die Precision-Scores werden in der \myRef{Tabelle}{precision-tabelle} zusammengefasst.

\begin{table}[!ht]
    \centering
    \label{precision-tabelle}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Precision & Hit in Five & Hit in Three & Hit in One \\ \hline
        BM25 & 0,56 & 0,5 & 0,39 \\ \hline
        Hybrid & 0,56 & 0,52 & 0,35 \\ \hline
        Semantic & 0,15 & 0,13 & 0,07 \\ \hline
    \end{tabular}
    \caption[Precision]{Precision der Suchfunktionen}
\end{table}

Es ist auffällig, dass der Precision-Score der semantischen Suche wesentlich schlechter ist als der Precision-Score der BM25 Suche.
Dies könnte sich wie folgt erklären lassen.
Bei der Implementierung der Suche wurden alle H1-Header, H2-Header und Paragraphen eines Dokuments zusammengefasst in jeweils einen String für das entsprechende HTML-Tag.
Der Algorithmus der Suchfunktion vergleicht nun den String der Sucheingabe mit den Strings in den Dokumenten.
Also mit dem String des Titels, dem String der H1-Header, dem String der H2-Header und dem String der Paragraphen.
Weil die einzelnen Inhalte der HTML-Tags zusammengefasst werden, können diese Strings größer werden als die Sucheingabe, welche in der Regel zwei bis drei Wörter umfasst.
Dadurch, dass die Sätze einen großen Größenunterschied haben, fällt entsprechend auch der Score der Dokumente niedrig aus.
Da nun alle Scores niedrig sind kann die Suchfunktion nicht so leicht das passendste Dokumente finden.
Dadurch werden zum Teil weniger relevante Dokumente gefunden und der Precision-Score fällt besonders niedrig aus.\\

Auch das Postprocessing könnte eine Auswirkung auf den Precision-Score der semantischen Suche haben.
Für Bag of Words Modelle, wie BM25, ist es typisch ein Stemming vor der Indizierung durchzuführen.
Zum einen hat dies positive Auswirkungen auf den Precision-Score.
Zum sorgt dies dafür, dass verschiedene Tokens als das gleiche Wort betrachtet werden, auch wenn sie nicht in der gleichen morphologischen Form stehen.
Die Details zu dieser Problematik wurden bereits in Kapitel \ref{chap:volltext-indizierung} erläutert.
Für die semantische Suche könnte ein Stemming allerdings negative Auswirkungen haben.
Denn bei dem Training eines Sentence Transformers werden ganze Sätze betrachtet und nicht nur die Stammformen der einzelnen Wörter.
Das bedeutet, dass der Sentence Transformer die Wörter nicht mehr korrekt semantisch zuordnen kann.
Dies wirkt sich wiederum negativ auf den Precision-Score der semantischen Suche aus.\\

Die Hypothese ist also, dass der Precision-Score der semantischen Suche nur aus dem Grund gering ist, weil neben dem Title-Tag auch die weiteren Tags indiziert werden.
Um diese Hypothese zu überprüfen wurde die Studie ein weiteres Mal durchgeführt.
Dieses Mal wurden lediglich die Title-Tags indiziert.
Das Title-Tag ist für jedes Dokument nur einmal vorhanden.
Außerdem sind die Titelsätze der Dokumente wesentlich kürzer als ganze Paragraphen.
Mit dieser Konfiguration sollte der Größenunterschied des Sucheingabe-Strings und der Titel-Strings kein entscheidender Faktor mehr sein.
Das Ergebnis dieser Konfiguration ist in \myRef{Tabelle}{precision-tabelle-2} dargestellt.\\

\begin{table}[!ht]
    \centering
    \label{precision-tabelle-2}
    \begin{tabular}{|l|l|l|l|}
        \hline
        Precision & Hit in Five & Hit in Three & Hit in One \\ \hline
        BM25 & 0,54 & 0,5 & 0,39 \\ \hline
        Hybrid & 0,54 & 0,48 & 0,35 \\ \hline
        Semantic & 0,28 & 0,22 & 0,11 \\ \hline
    \end{tabular}
    \caption[Precision (2. Durchführung)]{Precision der Suchfunktionen (2. Durchführung der Studie)}
\end{table}

In dieser Konfiguration hat die semantische Suche eine bessere Precision.
Die Hit in Five Precision liegt bei 0,28 für die Konfiguration, welche nur den Titel der Dokumente indiziert, gegenüber 0,15 bei einer vollständigen Indizierung.
Die Verbesserung der Precision kann mit drei Faktoren begründet werden.
Zum einen die beiden Hypothesen, welche mit dieser Beobachtung untersucht werden sollten.
Also zum einen der Größenunterschied von Sucheingabe-String und den Strings, welche indiziert werden.
Zum anderen die Tatsache, dass sich das Postprocessing der Dokumente negativ auf die Fähigkeit des Sentence Transformers auswirkt, die Semantik der Wörter zu verstehen.
Außerdem besteht die Möglichkeit, dass die Inhalte der Dokumente leicht von den Titeln der Dokumente abweichen können.
Das würde bedeuten, dass für den gegebenen Versuchsaufbau der Titel besser geeignet ist, um die gewünschten Dokumente zu finden, als das gesamte Dokument.
Die Precision für die BM25-Suche ist bei vollständig indizierten Dokumenten besser als bei alleiniger Beachtung des Titels.
Lediglich die semantische Suche hat sich bei der zweiten Konfiguration verbessert.
Das bedeutet, dass sich der gezeigte Effekt der Verbesserung der Suchfunktion tatsächlich mit der Hypothese begründen lässt, dass die Länge der Strings ein zu berücksichtigender Faktor ist.
Da die semantische Suche weiterhin schlechter ist als die BM25-Suche, scheint es aber weitere Faktoren zu geben, welche zu berücksichtigen sind.\\

Eine mögliche Erklärung für die weiterhin schlechten Ergebnisse der semantischen Suche ist die Tatsache, dass es sich bei dem verwendeten Datensatz um eine Closed-Domain handelt.
Das bedeutet, dass der Textcorpus Fachbegriffe beinhaltet, welche der Transformer mit hoher Wahrscheinlichkeit nicht kennt.
Dementsprechend schwer fällt es dem Transformer folglich, passende Vektoren zu generieren, welche die Semantik von den domänenspezifischen Fachbegriffen abbilden.
In \myRef{Kapitel}{chap:ausblick} wird auf Ansätze in der Literatur eingegangen, welche dieses Problem versuchen zu lösen.

\section{Diskussion des Studienaufbaus}
Wie bereits beschrieben gibt der Recall an, wie viele der relevanten Dokumente gefunden wurden.
Die Precision gibt lediglich an, wie viele der Dokumente, welche gefunden wurden, relevant sind.
Eine optimale Suchfunktion würde per Definition alle Dokumente finden, welche relevant sind, und keine irrelevanten Dokumente.
Umgekehrt ist eine schlechte Suchfunktion eine Suchfunktion, welche keine relevanten Dokumente findet, sondern nur irrelevante.
Dabei spielt es für den Nutzer aber keine Rolle, ob irrelevante Dokumente gefunden wurden.
Die Tatsache, dass die relevanten Dokumente nicht gefunden wurden, machen die Suchfunktion für den Nutzer nicht benutzbar. 
Die Studie untersucht die Precision der Suchalgorithmen.
Auf Grundlage der obigen Argumentation zeigt sich, dass der Recall-Score wichtiger ist als der Precision-Score.
Denn ist der Recall gering, dann ist die Suchfunktion nicht benutzbar.
Eine Suchfunktion mit einem hohen Recall, aber eine niedrigen Precision könnte dagegen viele relevanten Dokumente finden, aber auch viele irrelevante Dokumente.
Solange die relevanten Dokumente in der Liste weiter oben dargestellt werden, ist diese Zusammensetzung aus Precision und Recall gut.\\

Die Studie hat die Precision für das erste, die ersten drei und alle fünf Dokumente gemessen.
Die Studie hat allerdings keinen Recall gemessen.
% TODO: Warum ist der Recall so wichtig?
Grund dafür ist die Tatsache, dass um den Recall messen zu können, alle relevanten Dokumente für eine Sucheingabe bekannt sein müssen.
Um für eine Studie a priori Sucheingaben zu bestimmen, und alle Dokumente, welche für diese Sucheingabe relevant sind, müsste der gesamte Datensatz bekannt sein.
Da der Datensatz mehrere hundert Dokumente beinhaltet ist dies nicht möglich.
Und auch wenn der gesamte Datensatz bekannt wäre, dann müsste trotzdem eine Entscheidung darüber getroffen werden, welche Dokumente relevant sind und welche nicht.
Das wäre wiederum eine subjektive Entscheidung, sodass die Validität des Ergebnisses anzweifelbar wäre.

% TODO: Welche Auswirkungen hatte die Technologie auf die Performance