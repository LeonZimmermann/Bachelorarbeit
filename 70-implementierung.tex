\chapter{Implementierung}

Die Architektur von Suchmaschinen besteht aus drei Komponenten.
Aus dem Crawling, der Indexierung und der Suchfunktion selbst.
Das Crawling ist zuständig für das Finden von Websites.
Die Indizierung ist zuständig für das optimale Speichern der Informationen der Websites, und die Suche ist zuständig für das Verstehen der Nutzeranfrage und die Abfrage der relevantesten Informationen aus dem Index, sowie dessen Verarbeitung und Darstellung.
Im Folgenden wird der Begriff Dokument verwendet, um die Dateien zu beschreiben, welche durch einen Crawler gesucht und durch den Index verarbeitet werden.
Unter Dokumenten können hierbei auch eine Website verstanden, welche durch einen Webcrawler durchsucht werden.

\section{Crawling}
Bei der Implementierung eines Systems für eine Suchfunktion benötigt das System zunächst einen Datensatz von Dokumenten, welche überhaupt grundsätzlich über die Suchfunktion gefunden werden können.
Dieser wird mithilfe eines Crawlers aufgebaut.
Ein Crawler ist ein Algorithmus, welcher ein Dokument als Startpunkt bekommt, und anhand dessen neue Dokumente findet.
Der Algorithmus analysiert dazu das Dokument auf Links, welchen der Algorithmus anschließend folgt.
Die neuen Dokumente werden durch den Index verarbeitet und wiederum auf neue Links analysiert.
Dieses Verfahren kann beliebig lange und beliebig rekursiv durchlaufen werden, um den Index zu erweitern.
Neben dem Crawling können Indizes befüllt werden, indem eine Liste von Dokumenten übergeben werden, welche dem Index hinzugefügt werden sollen.

\section{Indizierung}
Bei der Indexierung werden Wörter und Tokens mit Dokumenten assoziiert.
Dazu werden Wörter aus gecrawlten Dokumenten extrahiert und in die Datenbank geschrieben.
Den Dokumenten werden IDs zugeordnet und diese IDs werden den Wörtern zugeordnet.
Dem Index werden nun weitere Informationen hinzugefügt, wie die Wortfrequenz.
Die Wortfrequenz gibt an, wie oft ein Wort in einem Dokument vorkommt.
Es wird auch gespeichert, an welchen Stelle des Dokuments das Wort vorkommt, und auch in wie vielen Dokumenten ein Wort vorkommt.
Diese Art der Indizierung wird auch als invertierter Index bezeichnet, weil den Wörtern bzw.
Tokens die Dokumente zugeordnet werden, und nicht umgekehrt.
Wenn der Nutzer nun ein Keyword in die Suche eingibt, dann sind diese Keywords oft bereits im Index vorhanden, sodass die Dokumente, welche diese Keywords beinhalten einfach dem Index entnommen werden können.
Aufgabe der Suche ist es anschließend die Ergebnisse aufzubereiten.

\subsection*{Volltext-Indizierung}
Bei der Indizierung der Wörter besteht die Problematik, dass gleiche Wörter in unterschiedlichen Formen existieren können.
So stammen „Heizung“ und „heizen“ beide von dem gleichen Wortstamm „heiz“ ab.
Um bei der Indizierung Speicherplatz zu sparen, können Wörter auf diesen Wortstamm reduziert werden, damit sie als ein einziges Wort betrachtet werden können.
Die Bildung des Wortstamms wird auch als Stemming bezeichnet.
Beim Stemming kann es jedoch zu Overstemming und Understemming kommen.
Overstemming bedeutet, dass zwei Wörter, die eigentlich nichts miteinander zu tun haben, also nicht semantisch gleich sind, den gleichen Wortstamm besitzen und als ein Wort betrachtet werden.
Ein Beispiel hierfür sind die Wörter „Wand“ und „wandere“, wie in „ich wandere“.
Beide besitzen den Wortstamm „wand“ und werden entsprechend als ein Wort betrachtet.
Understemming bedeutet, dass zwei Wörter, die eigentlich etwas miteinander zu tun haben, also semantisch gleich sind, nicht den gleichen Wortstamm besitzen und dadurch als zwei verschiedene Wörter betrachtet werden.
Ein Beispiel hierfür sind die Wörter „absorbieren“ und „Absorption“, welche die Wortstämme „absorb“ und „absorp“ besitzen.
Es gibt Techniken zur Vermeidung solcher Probleme, wie der Einsatz vollständiger morphologischer Analysekomponenten.
Hierauf soll aber nicht weiter eingegangen werden.
Zur Implementierung eines Volltext-Index werden Dokumente und Wörter in einer m x n Matrix angeordnet, wobei m die Anzahl der Dokumente ist und n die Anzahl der Wörter.
Die Werte in dieser Matrix werden anhand einer ausgewählten Metrik bestimmt.
Eine oft verwendete Metrik ist das Verhältnis der Wortfrequenz mit der invertierten Dokumentfrequenz.
Die Wortfrequenz gibt dabei an, wie häufig das in dem Dokument vorkommt.
Die Dokumentfrequenz gibt an, in wie vielen Dokumente ein Wort vorkommt.
Das Verhältnis gibt damit an, wie charakteristisch das Wort für ein bestimmtes Dokument ist.
Wenn das Wort charakteristisch für ein Dokument ist, dann kommt es in diesem Dokument häufig vor, aber in anderen Dokumenten nur selten.
Wenn das Wort nicht charakteristisch für das Dokument ist, dann kommt es in anderen Dokumenten genauso häufig oder häufiger vor als in diesem Dokument.
Die Metrik wird auch als tf-idf-Wert bezeichnet.
TODO Rechnung besser verstehen
Wir können uns nun einen m-dimensionalen Raum vorstellen, in dem jedes Dokument eine Dimension darstellt.
Jedes Dokument liegt räumlich auf der Achse der eigenen Dimension.
Wenn der Nutzer nun 
Die Matrix lässt sich konzeptionell so verstehen, dass sie aus n Merkmalsvektoren besteht.
Verwenden wir die eben beschriebene Metrik, dann gibt jeder Merkmalsvektor eine Gewichtung an, wie charakteristisch ein Wort für die verschiedenen m Dokumente ist.
Nun kann der Nutzer Keywords in die Suche eingeben.
Diese Keywords 

\section{Suffix-Trees}
TODO

\section{N-Gramm}
Ein N-Gramm ist das Ergebnis der Spaltung eines Textes in N Fragmente.
Fragmente können dabei Buchstaben, Wörter, Phoneme, Morpheme etc. sein.
N-Gramme können bei der Indizierung verwendet werden und werden auch in Algorithmen von Stemming und Information Retrieval verwendet, außerdem werden durch N-Gramme Rechtschreibprüfungen oder die Suche nach ähnlichen Wörtern ermöglicht.
Besteht ein Text aus einem Fragment, dann wird von einem Monogramm geredet.
Besteht es aus zwei, wird es Bigramm genannt.\\

Um Algorithmen wie eine Rechtschreibprüfung zu ermöglichen, wird neben N-Grammen auch Ähnlichkeitsmaß benötigt, welches beschreibt, wie ähnlich sich zwei Terme sind.
Das Wort Term ist an dieser Stelle synonym mit Wort zu verstehen.
Der Dice-Algorithmus ist ein Ähnlichkeitsmaß für Terme.
Er zerlegt zwei Terme in dessen N-Gramme und bestimmt anhand dieser die Ähnlichkeit der beiden Terme.
Der Dice-Algorithmus lautet wie folgt:\\

TODO Equation\\

Nehmen wir beispielsweise die beiden Terme a=“Wort“ und b=“Wirt“.
Verwenden wir in diesem Beispiel Bigramme, dann ergeben sich für a folgende Bigramme: „\$W“, „Wo“, „or“, „rt“, „t\$“.
Für b ergeben sich: „\$W“, „Wi“, „ir“, „rt“, „t\$“.
Die Schnittmenge der beiden Mengen, also die Bigramme, welche sowohl in a als auch in b vorhanden sind, lauten „\$W“, „rt“, „t\$“.
Es sind also drei Stück.
Das bedeutet der Zähler der Funktion ergibt 2*3 also 6.
Die Anzahl der Bigramme in a und b sind jeweils 5, also ist der Nenner 5+5, also 10.
Dann ist das Ähnlichkeitsmaß der beiden Wörter bei der Verwendung von Bigrammen 6/10 (60\%).
Bei der Verwendung von Trigrammen besitzt a „\$\$W“, „\$Wo“, „Wor“, „ort“, „rt\$“, „t\$\$“ und b „\$\$W“, „\$Wi“, „Wir“, „irt“, „rt\$, „t\$\$“.
Es gibt drei gemeinsame Trigramme.
Das Ergebnis ist (2*3)/(6+6) also 50\%.
An diesem Beispiel lässt sich zeigen, dass sich das Ergebnis des Ähnlichkeitsmaß verändert, wenn ein anderes N-Gramm verwendet wird.

\section{Information Extraction}
Information Extraction ist der Prozess des Extrahierens von Informationen aus Dokumenten.
Dazu müssen zunächst die Dokumente gefunden werden, welche die passenden Informationen enthalten.
Dieser Schritt wird Information Retrieval genannt.
Das Extrahieren von Informationen mit einer Vielzahl von Algorithmen und erfolgt in mehreren Schritten.
Der erste Schritt von Information Extraction ist das Einteilen des Textes in Sätze.
Dieser Schritt wird Sentence Segmentation genannt.
Anschließend werden aus jedem Satz die einzelnen Wörter extrahiert (Word Tokenization) und kategorisiert als Nomen, Verb, Objekt, Pronomen etc. (Part-of-Speech Tagging).
Für das Part-of-Speech Tagging gibt es zwei Implementierungsansätze, rule-based anhand von Grammatikregeln und statistic-based.
Es folgt Syntactic Parsing , also das Generieren eines Syntaxbaumes, welcher den Text repräsentiert.
Zuletzt wird mithilfe von Coreference Resolution identifiziert, welche Stellen im Text sich auf die gleichen Entitäten beziehen.
Ein Beispiel für Coreference Resolution ist, dass ein System versteht, dass mit „Angela Merkel“ und „Merkel“ oder auch „A.
Merkel“ die gleiche Person gemeint ist.
Mithilfe dieser Schritte können Key Phrase Extraction, Named Entity Recognition, Entity Disambiguation, Relation Extraction und Event Extraction implementiert werden.
Key Phrase Extraction ist das Identifizieren von Schlüsselwörtern im Text, welche für diesen Text charakteristisch sind und welche den Text klassifizieren.
Ein Rezept für das Backen von Schokoladenkeksen könnte beispielsweise mithilfe von Schlüsselwörtern, wie „Backen“, „Kekse“, „Schokolade“ beschrieben werden.
Named Entity Recognition ist das Identifizieren von Personen, Orten und Datumsangaben.
Entity Disambiguation ist das Auseinanderhalten von Entitäten, welche die gleichen oder ähnliche Wörter besitzen.
Unter dem englischen Wort „apple“ lässt sich nicht nur ein tatsächlicher Apfel verstehen, sondern es könnte sich auch um das Unternehmen apple handeln.
Welche Bedeutung des Wortes die korrekte ist, erschließt sich aus dem Kontext.
Das ist die Aufgabe von Entity Disambiguation.\\

TODO Image\\

Teilbereiche von Informationsextraktion sind Named Entity Recognition, Named Entity Linking, Relation Extraction, Knowledge Base Reasoning.
Informationsextraktion ist für viele Anwendungsfälle der erste Schritt in einer größeren Pipeline.
Das könnte z.B.
ein Chatbot sein oder ein Übersetzer.
Named Entity Recognition ist die Erkennung von Eigennamen.
Named Entity Linking ist der Prozess, eine bestimmte Entität eine einzigartige Identität zu geben.
Die Entität wird auf eine Target Knowledge Base gemappt, welche weitere Informationen über ebendiese Entität enthält.
Relation Extraction erkennt Beziehung zwischen Entities.
Die Beziehung kann durch ein Subjekt Prädikat Objekt Triple repräsentiert werden.
Abgrenzung von Information Extraction und Information Retrieval.
Information Retrieval selektiert relevante Dokumente.
Information Extraktion extrahiert Informationen aus einem gegebenen Dokument.
Die Klassifikation von Texten wird auch als Topic Modeling bezeichnet.
Es wird die Häufigkeit von Wörtern aus bestimmten Kategorien bestimmt und damit ein Thema zugeordnet.
Dieses Verfahren könnte zur Ermittlung von Labels verwendet werden.
Die Labels gehen dabei über die Inhalte des Textes hinaus.
Ein Algorithmus zum Topic Modeling ist Explicit Semantic Analysis (ESA).
Language Modelling ist ein Task von NLP, welcher es ermöglicht Texte zu übersetzen, Spellchecking, Spracherkennungen und Handschrifterkennung zu implementieren.\\

TODO Image