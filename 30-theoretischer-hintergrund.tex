\chapter{Konzeption der Suchfunktion}
Eine einfache Implementierung einer Suchfunktion kann aus drei Komponenten bestehen.
Aus dem Crawling, dem Index und dem Suchalgorithmus.
Das Crawling ist zuständig für das Finden von Dokumenten\cite{Castillo_2005}.
Der Index speichert Informationen der Dokumente, und die Suche ist zuständig für das Verstehen der Nutzeranfrage und die Abfrage der relevantesten Informationen aus dem Index, sowie dessen Verarbeitung und Darstellung.
Der Begriff Dokument wird hier verwendet, um die Dateien zu beschreiben, welche durch einen Crawler gesucht und durch den Index verarbeitet werden.
Unter Dokumenten können auch eine Website verstanden, welche durch einen Webcrawler durchsucht werden.
Es wird im Folgenden zunächst die Funktionsweise des Crawlings erläutert.
Danach werden zwei Möglichkeiten der Indizierung beschrieben.
Zuerst die Volltext-Indizierung, dann die Vektor-Indizierung.
Dann werden die verschiedenen Arten von Suchalgorithmen erläutert.
Und zuletzt wird auf Grundlage der dargestellten Informationen eine Konzeption zur Implementierung einer neuen Suchfunktion hergeleitet.

\section{Crawling}
Für die Implementierung einer Suchfunktion wird zunächst ein Datensatz von Dokumenten benötigt, welche über die Suchfunktion gefunden werden können.
Dieser wird mithilfe eines Crawlers aufgebaut.
Ein Crawler ist ein Algorithmus, welcher Techniken aus dem Natural Language Processing nutzt, um Informationen aus einem Dokument zu extrahieren.\cite{Khder_2021}
Implementierungen können reguläre Ausdrücke verwenden, um die Informationen zu extrahieren, oder auch fortgeschrittenere Verfahren, wie Abstract Syntrax Trees.
Im Falle von Websites kann der Crawler Hyperlinks zu weiteren Websites extrahieren.
Damit kann der Algorithmus sukzessive den Datensatz von Dokumenten befüllen.
Die neuen Dokumente werden durch den Index verarbeitet und wiederum auf neue Links analysiert.
Dieses Verfahren kann beliebig lange und beliebig rekursiv durchlaufen werden, um den Index zu erweitern.
Neben dem Crawling können Indizes befüllt werden, indem eine Liste von Dokumenten übergeben werden, welche dem Index hinzugefügt werden sollen.

\section{Indizierung}
Die Indizierung von Dokumenten kann sowohl mit einer Volltext-Indizierung oder auch einer Vektor-Indizierung umgesetzt werden.
Ein Volltextindex bestimmt den Score anhand von Ausschnitten aus dem Volltext.
Wenn diese bestimmte Kriterien erfüllen, dann wird das Dokument bei der Suche gefunden, andernfalls nicht.
Ein Vektorindex berechnet Vektoren anhand des Ursprungstextes.
Ein Vektorindex wird auch als Vector Space Model (VSM) bezeichnet.
Die Berechnung der Vektoren kann mithilfe von tf-idf, BM25, word2vec, Latent Semantic Embeddings oder auch mit Transformers erfolgen.

Das Ziel der Indizierung ist für jeden Algorithmus der gleiche: Für eine gegebene Sucheingabe sollen die relevantesten Dokumente gefunden werden.
Die Algorithmen unterscheiden sich darin, wie sie diese Relevanz berechnen.
Sie werden im Folgenden näher erläutert.

\subsection{Volltext-Indizierung}
Eine Möglichkeit zur Umsetzung einer Volltext-Indizierung ist der invertierte Index.
Die Dokumente werden in einer Datenbank abgelegt.
Anschließend wird der Index aufgebaut, indem alle Wörter extrahiert werden, welche in den Dokmenten vorkommen.
Nun werden diese Wörter in eine Liste geschrieben, und jedem Wort wird zugeordnet, in welchem Dokument sich dieses Wort wiederfinden lässt.
Diese Liste wird invertierter Index genannt, weil nicht die Wörter den Dokumenten zugeordnet sind, sondern die Dokumente den Wörtern.
Es wird ebenfalls gespeichert, an welchen Stelle des Dokuments das Wort vorkommt, und auch in wie vielen Dokumenten ein Wort vorkommt.\\

Bei der Indizierung der Wörter besteht nun die Problematik, dass gleiche Wörter in unterschiedlichen Formen existieren können.
So stammen \textit{Heizung} und \textit{heizen} beide von dem gleichen Wortstamm \textit{heiz} ab.
Um bei der Indizierung Speicherplatz zu sparen, können Wörter auf diesen Wortstamm reduziert werden, damit sie als ein einziges Wort betrachtet werden können.
Die Bildung des Wortstamms wird als Stemming bezeichnet.
Beim Stemming kann es jedoch zu Overstemming und Understemming kommen.
Overstemming bedeutet, dass zwei Wörter, die eigentlich nichts miteinander zu tun haben, also nicht semantisch gleich sind, den gleichen Wortstamm besitzen und als ein Wort betrachtet werden.
Ein Beispiel hierfür sind die Wörter \textit{Wand} und \textit{wandere}, wie in \textit{ich wandere}.
Beide besitzen den Wortstamm \textit{wand} und werden entsprechend als ein Wort betrachtet.
Understemming bedeutet, dass zwei Wörter, die eigentlich etwas miteinander zu tun haben, also semantisch gleich sind, nicht den gleichen Wortstamm besitzen und dadurch als zwei verschiedene Wörter betrachtet werden.
Ein Beispiel hierfür sind die Wörter \textit{absorbieren} und \textit{Absorption}, welche die Wortstämme \textit{absorb} und \textit{absorp} besitzen.
Es gibt Techniken zur Vermeidung solcher Probleme, wie der Einsatz vollständiger morphologischer Analysekomponenten.
Hierauf soll aber nicht weiter eingegangen werden.\\

\subsection{Vektor-Indizierung}
Neben einer Volltext-Indizierung können Dokumente in Form von Vektoren indiziert werden.
Dazu werden Dokumente zunächst, wie auch bei der Volltext-Indizierung gecrawlt.
Anschließend durchlaufen die Inhalte der Dokumente ein Preprocessing.
Dieses kann je nach Implementierung variieren.
Das Kapitel \textit{Einspielen der Daten in Weaviate} beschreibt, wie in der hier aufgeführten Implementierung das Preprocessing durchgeführt wird.
Das Preprocessing hat den Zweck die Daten an das Schema der Datenbank anzupassen und die Qualität der Daten zu erhöhen.
Außerdem sorgt es für eine kürzere Indizierungszeit.\\

Nach dem Preprocessing werden durch einen Transformer für die Inhalte der Dokumente Vektoren berechnet.
Ein Transformer wird mithilfe von Trainingsdaten darauf trainiert, Vektoren für Wörter zu generieren.
Das trainierte Modell wird nach dem Preprocessing durchlaufen.
Zuletzt werden die Daten in einer Vektordatenbank gespeichert.
Eine Vektordatenbank speichert Daten, wie eine dokumentenbasierte Datenbank.
Dort werden nun sowohl die rohen Daten als auch die Vektoren gespeichert, welche von dem Transformer berechnet wurden.
Die Vektoren haben den Vorteil, dass die Daten in der Datenbank nicht linear gespeichert sind.
Sie sind in einem n-dimensionalen Raum gespeichert, mit dessen Hilfe die semantische Nähe zwischen Dokumenten ausgedrückt werden kann.
Das funktioniert auf die gleiche Weise, wie bereits in dem Kapitel \textit{Semantic Search} beschrieben.

\subsection{Scoring Algorithmen}
TODO: Bag of words

TODO: Überarbeiten
Zur Implementierung eines Volltext-Index werden Dokumente und Wörter in einer m x n Matrix angeordnet, wobei m die Anzahl der Dokumente ist und n die Anzahl der Wörter.
Die Werte in dieser Matrix werden anhand einer ausgewählten Metrik bestimmt.
Eine oft verwendete Metrik ist tf-idf.\\

\(tf-idf(t,D)=tf(t,D)*idf(t)\)

\(tf(t,D)=\frac {\#(t,D)}{\max _{t'\in D}\#(t',D)}\)

\(idf(t)=\log {\frac {N}{\sum_{D:t\in D}1}}\)\\

TODO: Quelle

Der tf-Teil steht für \textit{term frequency} und wird berechnet, indem für das jeweilige Dokument bestimmt wird, wie häufig das Wort in dem Dokument vorkommt.
Damit die Metrik nicht abhängig von der Länge des Dokumentes ist, wird dieser Wert durch die insgesamte Anzahl der Vorkommnisse des Wortes dividiert.
Damit ist diese Metrik relativ zur insgesamten Anzahl der Vorkommnisse des Wortes, und nicht absolut.
Der idf-Teil steht für \textit{inverse document frequency}.
Er wird berechnet indem die insgesamte Anzahl der Dokumente durch die Anzahl der Dokumente dividiert wird, welche das Wort enthalten.
Das Ergebnis des dividierens wird an die Logarithmus-Funktion übergeben, sodass am Ende der idf-Wert berechnet wurde.
Die beiden Werte werden miteinander multipliziert.
Das Ergebnis ist die tf-idf-Metrik.

Wenn das Wort charakteristisch für ein Dokument ist, dann kommt es in diesem Dokument häufig vor, aber in anderen Dokumenten nur selten.
Wenn das Wort nicht charakteristisch für das Dokument ist, dann kommt es in anderen Dokumenten genauso häufig oder häufiger vor als in diesem Dokument.\\

Der BM25 Algorithmus ist eine Erweiterung der tf-idf Metrik.
TODO: BM25

\section{Cosine Similarity}
TODO: 

\section{Word Embeddings}
Word Embeddings sind eine Vektorrepräsentation von Wörtern.
Zweck von Word Embeddings ist es, semantische Ähnlichkeiten zwischen Wörtern zu verstehen.
Eine einfache Implementierung von Word Embeddings könnte sein, jedem Wort eine zufällige Zahl zuzuordnen.
Wenn die Zahlen zufällig sind, werden damit allerdings keine semantischen Ähnlichkeiten kodiert.
Um herauszufinden, welche Wörter eine semantische Ähnlichkeit besitzen.

\section{word2vec}
Word2vec ist ein zwei-Layer neuronales Netzwerk, welches die Semantik von Wörtern lernt.
Für jedes Wort, für welches eine Vektorrepräsentation bestimmt werden soll, benötigt das neuronale Netzwerk einen Input.
Das erste Layer des neuronalen Netzwerkes besteht aus Aktivierungsfunktionen.
Jede Node des Layers summiert die Inputs.
Die Weights der Aktivierungsfunktionen sind am Ende die Vektorrepräsentationen der Wörter.
Die Anzahl der Nodes im Aktivierungslayer bestimmt die Anzahl der Dimensionen im Vektor.
Das nächste Layer summiert wiederum die Werte aus dem Aktivierungslayer und wendet eine SoftMax Funktion an.
Der Output des neuronalen Netzwerkes besteht, so wie der Input, aus so vielen Nodes, wie es Wörter gibt, welche mit dem Netzwerk repräsentiert werden sollen.
Der Output gibt an, mit welcher Wahrscheinlichkeit jedes der Wörter das Folgewort des gegebenen Inputs ist.\\ 

Das neuronale Netzwerk wird mithilfe von Backpropagation trainiert.
Dazu wird Training Data benötigt.
Dieses enthält Sätze, auf welche das neuronale Netzwerk trainiert werden soll.
Aufgabe des neuronalen Netzwerkes ist es, auf Basis eines gegebenen Wortes das nächste Wort im Satz zu bestimmen.
Für ein einfaches Beispiel könnte das Trainingsdatenset aus zwei Einträgen \textit{NLP ist interessant} und \textit{NLP ist komplizit} bestehen.
Da das Trainingsdatenset insgesamt aus vier verschiedenen Wörtern besteht, hat das neuronale Netzwerk genau vier Inputs.
Wenn nun das Folgewort für das Wort \textit{NLP} trainiert werden soll, dann bekommt das neuronale Netzwerk als Input die Werte \textit{1, 0, 0, 0}.
Die Eins bedeutet, dass das Wort NLP aktiviert ist.
Die Nullen bedeuten, dass die anderen Wörter nicht aktiviert sind.
Nun muss das neuronale Netzwerk für diesen Input den Output-Wert berechnen.
Für die Backpropagation wird die Cross-Entropy Loss Function verwendet.
Das Ergebnis ist, dass das trainierte neuronale Netzwerk für das Wört \textit{NLP} lernt, dass das nächste Wort mit der Wahrscheinlichkeit eins das Wort \textit{ist} ist.
Die gelernten Weights bilden einen Vektor, welcher in einer Vektordatenbank gespeichert werden kann.
Das Ergebnis ist eine Vektordatenbank, welche Informationen darüber enthält, welche Wörter semantisch ähnlich sind, und welche nicht.

\section{Sentence Embeddings}
TODO: Sentence Transformer, wie Bert

\section{seq2seq}
Ein seq2seq-Modell soll eine Sequenz auf eine andere Sequenz anhand von erlernten Regeln mappen.
Dazu wird die Input-Sequenz zunächst in eine modell-interne Sequenz encodiert, der Context Vector, und anschließend in die Output-Sequenz dekodiert.
Wenn eine Wortsequenz in eine andere Wortsequenz umgewandelt werden soll, dann müssen die Input-Wörter zunächst durch ein Word Embedding Layer verarbeitet werden, welches die Wörter in Word Embeddings konvertiert.
Auf das Word Embedding Layer folgen eine beliebige Anzahl von LSTM-Layers.
Die Anzahl der Nodes der LSTM-Layer entspricht der Anzahl der Nodes im Word Embedding Layer.
Zusätzlich kann jede Node in jedem LSTM-Layer mehrere LSTM Netzwerke besitzen.
Die Weights und Biases der LSTM Netzwerke bilden den Output und damit den Context Vector.
Die bisher beschriebene Architektur wird als Encoder bezeichnet.
Die LSTM Netzwerke des Encoders werden im Decoder gespiegelt.
Die Outputs des oberen LSTM Layers des Decoders werden in ein Fully Connected Layer gegeben und durchlaufen eine Softmax Funktion.
Der Output des Decoders sind die Wahrscheinlichkeiten, mit welchen die möglichen Wörter der Ziel-Sequenz dem Input entsprechen. 

\section{Suchalgorithmen}
Im Folgenden werden zuerst gängige Suchalgorithmen erklärt, welche in nahezu jeder Suchfunktion zu finden sind.
Danach wird im speziellen auf die strukturierte Suche und die semantische Suche eingegangen.
Die semantische Suche wird später bei der Implementierung einer Suchfunktion verwendet werden.

\subsection{Die gängigen Suchalgorithmen}
In dem Kapitel \textit{Evaluationsmethoden und -Kriterien} wurde bereits beschrieben, dass eine Suche das Qualitätskriterium der Konformität erfüllen sollte.
Das bedeutet, dass eine Suchfunktion die gängigen Arten von Suchanfragen unterstützen muss.
Dieses Kapitel soll die Arten von Suchanfragen vorstellen.
Es ist wichtig diese zu kennen, um das Qualitätskriterium später bei der Implementierung erfüllen zu können.
Es gibt die gängigen Suchalgorithmen Keyword Search, Phrase Search, Boolean Search, Field Search.\\

Eine Keyword Search durchsucht Dokumente nach der Sucheingabe des Nutzers.
Die Eingabe wird dabei nicht als ganzes betrachtet, sondern jedes Wort einzeln.
Für jedes Keyword werden Dokumente als Ergebnis angezeigt, wenn dieses in dem Dokument vorhanden ist.
Wenn ein Dokument mehrere der Keywords beinhaltet, wird dessen Relevanz höher eingeschätzt als für Dokumente, welche weniger Keywords enthalten.
Dokumente mit höherer Relevanz werden weiter oben in der Ergebnisliste angezeigt.\\

Eine Phrase Search ist die Suche nach Textausschnitten in Dokumenten.
Hier werden nicht mehrere Keywords einzeln betrachtet, sondern die gesamte Eingabe in das Suchfeld als eine Einheit.
Es reicht also nicht mehr aus, dass ein Dokument eines der Wörter enthält.
Es muss die gesamte Sucheingabe als ein String enthalten sein.\\

Die Boolean Search bietet die Möglichkeit einen boolschen Ausdruck als Sucheingabe zu machen.
Ein Beispiel dafür ist die Sucheingabe \textit{Dokumentation AND Angular}.
Die Sucheingabe bedeutet, dass die Suchfunktion nur Dokumente als Ergebnis darstellen soll, welche beide Keywords Dokumentation und Angular enthalten.
Die Boolean Search kann auch eine Phrase, wie bei der Phrase Search, beinhalten: \textit{"Dokumentation von Software" AND Angular}.
In diesem Beispiel werden nur Dokumente als Ergebnis nur angezeigt, wenn sie den gesamten String \textit{Dokumentation von Software} enthalten, sowie das Keyword \textit{Angular}.
Bei einer Boolean Search können die boolschen Operatoren \textit{AND}, \textit{OR}, \textit{NOT} beliebig kombiniert werden.\\

Eine Field Search sucht Dokumente anhand von Attributen.
Der Nutzer kann diese Attribute auswählen.
Wenn der Nutzer beispielsweise ein Dokument sucht, welches am 01.01.2005 erstellt wurde, dann kann die Eingabe der Suche so aussehen: \textit{erstelldatum: 01.01.2005}.
Es können beliebig viele Attribute verwendet werden, um die Suche einzugrenzen.
Neben der Verwendung der Attribute für die Suche selbst, können die Attribute komplementär zu einer anderen Art von Suche verwendet werden.
So kann eine Suchfunktion Buttons bereitstellen, über welche Filter festgelegt werden.
Nun kann eine Keyword Search durchgeführt werden, aber die gefundenen Dokumente werden mithilfe der Filter weiter eingeschränkt.
Neben diesen gängigen Suchalgorithmen gibt es weitere Suchalgorithmen, wie die strukturierte Suche und die semantische Suche.

\subsection{Die semantische Suche}
Unter einer semantischen Suche wird im Allgemeinen verstanden, dass die Suche nicht nur eine syntaktische Suche einer Zeichenkette ist, sondern auch Techniken, wie technische Analysen verwendet, um die Bedeutung der Sucheingabe nachzuvollziehen.\cite{Dengel_2012}
Eine semantische Suche hat den Zweck, die Ähnlichkeit und Beziehungen zwischen Wörtern zu verstehen.
Sie kennt Homonyme, Synoyme und Antonyme von Wörtern.  
So wird durch sie beispielsweise die Ähnlichkeit von den Wörtern \textit{rollout} \textit{deployment} abgebildet, und dass diese Wörter oft im gleichen Kontext verwendet werden.\\

Die technische Umsetzung einer semantischen Suche kann auf verschiedene Arten erfolgen.
Zum einen besteht die Möglichkeit ein explizites semantisches Netz heranzuziehen, und so die Zusammenhänge von Wörtern abzubilden.
Ein semantisches Netz ist eine Menge aus Aussagen in der Form \textit{Subjekt Prädikat Objekt}.
Anhand dieser Aussagen wird ein Graph aus Beziehungen zwischen Wörtern hergestellt.\cite{Lehmann}
TODO: Erläuterende Grafik + Beispiel.\\
Das hat den Vorteil, dass ein solches explizites semantisches Netz von Menschen erstellt wurde, und damit eine hohe Qualität der Daten einhergeht.
Denn an der Erstellung von semantischen Netzen sind mehrere Menschen beteiligt, welche zuerst einen Konsens über die Eigenschaften und Zusammenhänge von Wörtern schaffen müssen.
Der Nachteil dieser Methode ist der große Arbeitsaufwand, welcher mit der Erstellung eines solchen semantischen Netzes einhergeht.
Ein weiterer Nachteil ist die mögliche Unvollständigkeit, welche ein solches semantisches Netz besitzen könnte.
Eine Wissensdatenbank, welche für ein Projekt erstellt wurde, kann Definitionen von Begriffen beinhalten, welche in allgemeinen semantischen Netzen nicht vorhanden sind.
Bei der Umsetzung einer semantischen Suche mithilfe eines semantischen Netzes müsste also zuerst ein allgemeines semantisches Netz herangezogen werden, beispielsweise von DBPedia.
Anschließend müsste dieses semantische Netz um die neuen Definitionen, welche nur im Kontext des Projektes gelten, erweitert werden.\\

Eine weitere Möglichkeit zur Umsetzung einer semantischen Suche ist die Verwendung von Transformers und Vektordatenbanken.
Um zu verstehen, welche Wörter kontextuell zusammengehören, werden hier die Wörter in einem n-dimensionalen Raum positioniert.
Wörter, die sich sehr ähnlich sind, also im gleichen Kontext verwendet werden, haben in diesem n-dimensionalen Raum eine geringe Distanz zueinander.
Wörter, die sich eher unähnlich sind, wie \textit{"rollout" und "API"}, haben eine größere Distanz.
Der Vorteil einer semantischen Suche ist, dass der genaue Begriff, welcher gesucht wird nicht bekannt ist.
Wenn sich der Nutzer also über ein Thema informieren möchte, mit welchem er nicht gut vertraut ist, dann kann die semantische Suche hilfreich sein.
Denn der Nutzer kann nun einen Begriff eingeben, der zu dem Thema passt, und den er kennt.
Er findet anschließend Dokumente, welche vielleicht nicht genau diesen Begriff beinhalten, aber welche thematisch dennoch ähnlich sind.
Genau dieser Vorteil soll bei der Implementierung später genutzt werden.\\

Um eine semantische Suche zu implementieren, werden die Technologien von Transformern und Vektordatenbanken verwendet.
Ein Transformer erhält als Input eine große Menge an Text und mappt die einzelnen Wörter auf einen Vektor einer beliebigen Länge.
Der Vektor, der am Ende herauskommt, beschreibt die Position des Wortes in dem n-dimensionalen Raum.
Der Vektor beschreibt gewissermaßen, wie stark ein Wort in eine abstrakte Kategorie einzuordnen ist.
Jeder Wert im Vektor entspricht einer Kategorie.
Mithilfe der Vektoren können verschiedene Wörter hinsichtlich ihrer Ähnlichkeit analysiert werden.
Ähnliche Wörter habe eine große räumliche Nähe, während zwei Wörter, die in vollkommen unterschiedlichen Kontexten verwende werden eine sehr große Distanz im Raum besitzen.
Nehmen wir für ein Beispiel einen dreidimensionalen Raum an.
Die X-Achse ist beschriftet mit dem Wort „Tier“, die Y-Achse ist beschriftet mit dem Wort „Computer“ und die Z-Achse ist beschriftet mit dem Wort „Mensch“.
Nun geben wir einem Transformer das Wort „Katze“, und der Transformer berechnet einen dreidimensionalen Vektor, welcher das Wort „Katze“ im Raum positioniert.
Weil eine Katze ein Tier ist, ist der X-Wert des Vektors eins.
Der Wert eins bedeutet, dass das Wort vollständig zu dieser Kategorie gehört.
Da eine Katze überhaupt nichts mit einem Computer zu tun hat, ist der Y-Wert des Vektors 0.\\

Nun ist eine Katze kein Mensch, aber eine Katze ist ein Haustier von Menschen.
Es ist denkbar, dass die Wörter Katze und Mensch oft im gleichen Kontext verwendet werden, sodass der Wert bei 0,3 liegen könnte.
Damit der Transformer einen Vektor berechnen kann, braucht er eine Menge Daten.
Diese Daten erhält er aus vielen Texten.
Werden zwei Wörter oft im gleichen Text genannt oder kommen zwei Wörter in vielen Texten sehr nahe beieinander vor, dann geht der Transformer davon aus, dass die beiden Wörter ähnlich sind, und berechnet ähnliche Vektoren.
Zuvor müssen die Texte allerdings bereitgestellt werden.
Dazu kann beispielsweise das Internet gecrawlt werden.
Die Ergebnisse des Transformers werden in einer Vektordatenbank gespeichert.
Eine Vektordatenbank ist eine Datenbank, welche Vector Embeddings, also ein Objekt als Key und dessen Vektor als Value speichert.
Bei dem Objekt kann es sich um Wörter handeln, dann wird auch von Word Embeddings gesprochen.
Es können aber auch Daten andere Daten, wie Bilder, Videos oder Audio gespeichert werden.
Der Zweck von Vektordatenbanken ist es, Daten nicht einfach linear zu speichern, sondern in einem Raum.
Die Distanz zwischen zwei Einträgen in diesem Raum beschreibt dessen Ähnlichkeit.
Genau diese Informationen nutzen semantische Suchen.

\section{Wahl der Suchfunktion}
Die vergangenen Kapitel haben verschiedene Indizierungs- und Suchalgorithmen dargestellt.
Jeder dieser Algorithmen hat Vor- und Nachteile.
Diese werden in diesem Kapitel betrachtet, um die Entscheidung für die verwendeten Technologien der neuen Suchfunktion nachvollziehbar zu machen.\\

Die Confluence Suche verwendet laut Dokumentation Apache Lucene.\footnote{\url{https://confluence.atlassian.com/doc/ranking-of-search-results-1188406620.html}}
Laut der Dokumentation von Apache Lucene, verwendet dieses einen Scoring Algorithmus, welcher sowohl auf VSM als auch auf Boolean Models basiert.\footnote{\url{https://lucene.apache.org/core/2_9_4/scoring.html}}
Im Information Retrieval sind Boolean Models jene Scoring Algorithmen, welche auf boolscher Algebra basieren.
Aus der Dokumentation von Apache Lucene und aus dem genannten Paper geht ebensfalls hervor, dass die Vektoren des VSM mit tf-idf berechnet werden.

In einem Paper von Choudhary et. al. wird ein Document Retrieval System entwickelt.\cite{Choudhary_Guttikonda_Chowdhury_Learmonth_2020}
Das Document Retrieval System verwendet Bert zur Generierung von Embeddings.
Der Scoring Algorithmus des Systems kombiniert Bert und tf-idf, um den Score zu berechnen.
Laut dem Paper bietet eine Kombination aus tf-idf und Bert zur Implementierung eines Document Retrieval Systems signifikante Performance Verbesserungen gegenüber einem Document Retrieval System, welches lediglich tf-idf verwendet.
Die Performance Verbesserung wird in dem Paper an dem MS Marco Datensatz gemessen.\footnote{\url{https://microsoft.github.io/msmarco/}}\\

In einem Paper von Karpukhin et. al. wird Open-Domain Question Answering untersucht.\cite{Karpukhin_Oguz_Min_Lewis_Wu_Edunov_Chen_Yih_2020}
Dazu wird ein Dense Passage Retriever entwickelt.
Ein Dense Passage Retriever bestimmt den Textabschnitt eines gegebenen Textes, welcher mit größter Wahrscheinlichkeit eine Antwort auf eine gestellte Frage beinhaltet.
Der Input für einen Dense Passage Retriever sind Dokumente, welche zuvor mithilfe eines Scoring Algorithmus aus einem Index extrahiert wurden.
Das Paper zeigt, dass das entwickelte Passage Retrieval System besser darin ist relevante Textabschnitte zu finden als der BM25 Algorithmus. 
Das Paper nennt die semantische Verknüpfung von Synonymen und Paraphrasierungen mit unterschiedlichen Tokens als Vorteil gegenüber BM25 und auch TF-IDF.

Die semantische Suche ist bei dem Einspielen der Daten in die Datenbank langsamer, weil zunächst die Vektoren für die Daten generiert werden müssen.
Die Zeit für die Abfrage von Daten aus einem Vector Space Model wächst mit wachsender Anzahl von Datenpunkten nicht so schnell an, wie bei einem Volltext-Index.
Bei einem Volltext-Index müssen die übergebenen Keywords mit jedem Datensatz abgeglichen werden.
Bei einem Vector Space Model sind zwar mit steigender Zahl von Datensätzen mehr Punkte im Raum vorhanden, aber es müssen nicht für jeden Punkt Strings verglichen werden.
Der Algorithmus, welcher für die Ermittlung der passendsten Datenpunkte im Vector Space Model verwendet wird, lautet Approximate Nearest Neighbor (ANN).
Dieser basiert auf linearer Algebra.
TODO: Erklären, warum ANN schneller sein soll

TODO: Was gäbe es noch für Ansätze?

Bei der Implementierung sollen dabei keine semantischen Netze verwendet werden, sondern ein Vector Space Model.
TODO: Warum?

Neben den direkten Vorteilen einer semantischen Suche gegenüber Suchfunktionen, wie eine Keyword Search, gibt es weitere indirekte Vorteile eine semantischen Suche auf Grundlage von Vector Space Models.
Vector Space Models sind Multimodal.
Es können also in einem Vector Space Model nicht nur Informationen über ein Medium, wie Fließtext, gespeichert werden, sondern neben Fließtext können gleichzeitig auch andere Medien, wie Code, Bilder, Audiodateien etc. gespeichert werden.
Und anders als bei einer gewöhnlichen Datenbank, in welcher mehrere verschiedene Medien gespeichert werden können, beispielsweise eine relationale Datenbank, können Vector Space Models diese verschiedenen Medien miteinander semantisch verknüpfen.\\

Wenn ein Softwareentwickler ein bestehendes Feature anpassen muss, dann muss er zunächst den Einstiegspunkt im Code kennen.
Sind keine weiteren Informationen vorhanden, dann muss der Softwareentwickler dazu den Code manuell durchsuchen.
Mithilfe von Techniken aus dem Natural Language Processsing, wie Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA) und Vector Space Models kann dieser Prozess automatisiert werden.\cite{Dit_Revelle_Gethers_Poshyvanyk_2011}
Mithilfe eines gemeinsamen Index von den Inhalten der Wissensdatenbank und dem Quellcode lässt sich ein Zusammenhang zwischen Feature-Spezifikation und Klassen im Quellcode herstellen.
Die Herangehensweise erfordert eine Möglichkeit beide Datenquellen auf die gleiche Art und Weise zu indizieren.
Dazu werden LSI und Vector Space Models verwendet.\cite{Antoniol_Canfora_Casazza_DeLucia_2000}

Indem die Codebase, welche zu der Wissensdatenbank in Verbindung steht, ebenfalls indiziert wird, kann eine Traceability zwischen Code und Wissensdatenbank hergestellt werden.
Damit können Feature Location und Bug Localization umgesetzt werden.
So kann die Suchfunktion nicht nur zum Durchsuchen der Wissensdatenbank verwendet werden, sondern auch zum Durchsuchen der Codebase.
Und wenn in der Suchfunktion nach einem Feature gesucht wird, dann kann neben der Spezifikation in der Wissensdatenbank auch gleich der Einstiegspunkt im Code gefunden werden.
Damit verbessert sich wiederum die Wartbarkeit der Software, da das Ändern von Features oder korrigieren von Bugs erleichtert wird.\\
TODO: Quellen

\section{Weitere Ansätze}

\subsection{Retrieval Augmented Generation}
Neben einer Suchfunktion können System entwickelt werden, welche eine Question-Answering Funktionalität bieten.
Eine Question-Answering Funktionalität nimmt eine Frage des Nutzers entgegen, und liefert eine Antwort.
Solche Systeme basieren auf Retrieval Augmented Generation.
Das bedeutet, dass auf Grundlage der Frage des Nutzers zuerst eine Suche durchgeführt wird.
Diese Suche liefert relevante Dokumente zum Beantworten der Frage.
Nun werden die, für die Frage relevantesten, Stellen der Dokumente mithilfe eines Large Language Models extrahiert und umformuliert.
Das Ergebnis ist eine Antwort auf die Frage des Nutzers. 

\subsection{Document Classification und Filter verwenden}
TODO:

\subsection{Verbesserung der Suchfunktion durch Traceability (insb. für Feature Location und Bug Localization)}
TODO: